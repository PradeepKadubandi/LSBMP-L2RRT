{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from spatialsoftmax import SpatialSoftmax\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things that doesn't yet seem to be supported in pytorch:\n",
    "# padding = same\n",
    "# spatial softmax\n",
    "class AutoEncoder_Dynamics(nn.Module):\n",
    "    def __init__(self, img_res=32, z_dim=2, u_dim=2):\n",
    "        super(AutoEncoder_Dynamics, self).__init__()\n",
    "        \n",
    "        self.img_res = img_res\n",
    "        self.x_dim = img_res*img_res\n",
    "        self.z_dim = z_dim\n",
    "        self.u_dim = u_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('conv3', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('softmax', SpatialSoftmax(img_res, img_res, 8)),\n",
    "            ('fc1', nn.Linear(8*2, 256)), # Spatial softmax will result in 2 values per channel 1 along height and 1 along width.\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(p=0.5)),\n",
    "            ('fc2', nn.Linear(256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(256, self.z_dim))\n",
    "        ]))\n",
    "#         self.encoder = self.encoder.float()\n",
    "        self.dynamics = nn.Sequential(OrderedDict([\n",
    "            ('d_fc1', nn.Linear(self.z_dim + self.u_dim, 128)),\n",
    "            ('d_relu1', nn.ReLU()),\n",
    "            ('d_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('d_fc2', nn.Linear(128, 128)),\n",
    "            ('d_relu2', nn.ReLU()),\n",
    "            ('d_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('d_fc3', nn.Linear(128, 128)),\n",
    "            ('d_relu3', nn.ReLU()),\n",
    "            ('d_fc4', nn.Linear(128, self.z_dim)),\n",
    "            ('d_relu4', nn.ReLU())\n",
    "        ]))\n",
    "#         self.dynamics = self.dynamics.float()\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            ('dec_fc1', nn.Linear(self.z_dim, 512)),\n",
    "            ('dec_relu1', nn.ReLU()),\n",
    "            ('dec_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc2', nn.Linear(512, 512)),\n",
    "            ('dec_relu2', nn.ReLU()),\n",
    "            ('dec_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc3', nn.Linear(512, 512)),\n",
    "            ('dec_relu3', nn.ReLU()),\n",
    "            ('dec_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc4', nn.Linear(512, 512)),\n",
    "            ('dec_relu4', nn.ReLU())\n",
    "        ]))\n",
    "#         self.decoder = self.decoder.float()\n",
    "        self.environment = nn.Sequential(OrderedDict([\n",
    "            ('env_conv1', nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('env_relu1', nn.ReLU()),\n",
    "            ('env_flat1', nn.Flatten()),\n",
    "            ('env_fc1', nn.Linear(4 * self.x_dim, 512)),\n",
    "            ('env_relu1', nn.ReLU()),\n",
    "            ('env_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('env_fc2', nn.Linear(512, 512)),\n",
    "            ('env_relu2', nn.ReLU()),\n",
    "            ('env_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('env_fc3', nn.Linear(512, 512)),\n",
    "            ('env_relu3', nn.ReLU()),\n",
    "            ('env_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('env_fc4', nn.Linear(512, 512)),\n",
    "            ('env_relu4', nn.ReLU())\n",
    "        ]))\n",
    "#         self.environment = self.environment.float()\n",
    "        self.last_layer = nn.Linear(512 + 512, self.x_dim)\n",
    "#         self.last_layer = self.last_layer.float()\n",
    "    \n",
    "    def encode(self, x_t, x_tplus):\n",
    "        x_full = torch.cat((x_t, x_tplus), dim=0)\n",
    "        input_enc = torch.reshape(x_full, [-1, 1, self.img_res, self.img_res])\n",
    "        z_full = self.encoder(input_enc)\n",
    "        return x_full, z_full\n",
    "\n",
    "    def predict_dynamics(self, z_t, u_t):\n",
    "        input_dyn = torch.cat((z_t, u_t), dim=1) #TODO: Do I have to use torch.identity after concatenation? why/not?\n",
    "        z_hat_tplus = self.dynamics(input_dyn)\n",
    "        return z_hat_tplus\n",
    "        \n",
    "    def compute_grammian(self, z_t, u_t, z_hat_tplus, retain_graph_after_last_grad=True):\n",
    "        z_hat_tplus_zero = z_hat_tplus[:, 0]\n",
    "        z_hat_tplus_one = z_hat_tplus[:, 1]\n",
    "\n",
    "        grad_zh0_zt = autograd.grad(z_hat_tplus_zero, z_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_zero.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=True)[0]\n",
    "        grad_zh1_zt = autograd.grad(z_hat_tplus_one, z_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_one.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=True)[0]\n",
    "        grad_zh0_ut = autograd.grad(z_hat_tplus_zero, u_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_zero.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=True)[0]\n",
    "        grad_zh1_ut = autograd.grad(z_hat_tplus_one, u_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_one.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=retain_graph_after_last_grad)[0]\n",
    "\n",
    "        A = torch.stack([grad_zh0_zt, grad_zh1_zt], dim=1) # N x D_z_hat x D_z  (D_z_hat = D_z = 2)\n",
    "        B = torch.stack([grad_zh0_ut, grad_zh1_ut], dim=1) # N x D_z_hat x D_c  (D_c = 2)\n",
    "        c = self.__expand_dims(z_hat_tplus) - torch.bmm(A, self.__expand_dims(z_t)) - torch.bmm(B, self.__expand_dims(u_t))\n",
    "        AT = torch.transpose(A, 1, 2) # Preserve the batch dimension 0 and transpose dimentions 1 and 2\n",
    "        BT = torch.transpose(B, 1, 2)\n",
    "        \n",
    "        G = torch.bmm(A, torch.bmm(B, torch.bmm(BT, AT))) # N x D_z x D_z (remember D_z_hat = D_z)\n",
    "        offset_for_invertible = (0.0001 * torch.eye(G.size()[1])).expand_as(G)\n",
    "        with torch.no_grad(): # Is this needed? Probably not...\n",
    "            G_inv = torch.inverse(G + offset_for_invertible) # N x D_z x D_z\n",
    "            \n",
    "        return G_inv, A, c\n",
    "        \n",
    "    def forward(self, x_t, x_tplus, x_empty, u_t):\n",
    "        '''\n",
    "        x_t, x_tplus, x_empty must be of shape [N, C*H*W] where, N = batch_size, \n",
    "        C = Channels, H = Height, W = Width of image.\n",
    "        u is of shape [N, D_c] where D_c = Control Dimension.\n",
    "        '''\n",
    "        batch_size = u_t.size()[0]\n",
    "\n",
    "        x_full, z_full = self.encode(x_t, x_tplus)\n",
    "        \n",
    "        z_t = z_full[:batch_size, :]\n",
    "        z_tplus = z_full[batch_size:, :]\n",
    "\n",
    "        z_hat_tplus = self.predict_dynamics(z_t, u_t)\n",
    "        \n",
    "        input_dec = torch.cat((z_t, z_hat_tplus), dim=0) #TODO: Again, should I use torch.identity here?\n",
    "        output_dec = self.decoder(input_dec)\n",
    "        \n",
    "        x_empty_full = torch.cat((x_empty, x_empty), dim=0)\n",
    "        input_env = torch.reshape(x_empty_full, [-1, 1, self.img_res, self.img_res]) #TODO: identity?\n",
    "        output_env = self.environment(input_env)\n",
    "        \n",
    "        input_last = torch.cat((output_dec, output_env), dim=1)\n",
    "        x_hat_full = self.last_layer(input_last)\n",
    "        \n",
    "        return x_full, z_t, z_tplus, x_hat_full, z_hat_tplus\n",
    "        \n",
    "    def compute_loss(self, u_t, x_full, z_t, z_tplus, x_hat_full, z_hat_tplus, L2_weight):\n",
    "        '''\n",
    "        From a typical pytorch code principles, perhaps this should be a different class\n",
    "        than the net class itself but that's ok for now.\n",
    "        '''\n",
    "        G_inv, _, _ = self.compute_grammian(z_t, u_t, z_hat_tplus)\n",
    "\n",
    "        z_diff = self.__expand_dims(z_hat_tplus) - self.__expand_dims(z_tplus) # N x D_z x 1\n",
    "        z_diff_T = torch.transpose(z_diff, 1, 2) # N x 1 x D_z\n",
    "        \n",
    "        predict_loss_G = torch.sum(torch.abs(torch.bmm(z_diff_T, torch.bmm(G_inv, z_diff)))) # N x 1 before sum, scalar after sum\n",
    "        predict_loss_L2 = F.mse_loss(z_hat_tplus, z_tplus,  reduction='mean')\n",
    "        predict_loss = predict_loss_G * (1 - L2_weight) + predict_loss_L2 * L2_weight\n",
    "        \n",
    "        recon_loss = F.mse_loss(x_hat_full, x_full, reduction='mean')\n",
    "        total_loss = predict_loss + recon_loss\n",
    "        \n",
    "        return total_loss, predict_loss_G, predict_loss_L2, recon_loss\n",
    "    \n",
    "    def __expand_dims(self, input):\n",
    "        return input.unsqueeze(input.dim())\n",
    "        \n",
    "class CollisionChecker(nn.Module):\n",
    "    def __init__(self, img_res=32, z_dim=2, u_dim=2):\n",
    "        super(CollisionChecker, self).__init__()\n",
    "        self.img_res = img_res\n",
    "        self.x_dim = img_res*img_res\n",
    "        self.z_dim = z_dim\n",
    "        self.u_dim = u_dim\n",
    "        conv_filters = 10\n",
    "        padding=3\n",
    "        kernel_size=7\n",
    "        fc_dim = 128\n",
    "        self.imageConvLayer = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=conv_filters, kernel_size=kernel_size, padding=padding)), # kernel_size different than original\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(conv_filters, conv_filters, kernel_size, padding)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('conv3', nn.Conv2d(conv_filters, conv_filters, kernel_size, padding)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('flat1', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(conv_filters*self.x_dim, 4*fc_dim)),\n",
    "            ('relu4', nn.ReLU())\n",
    "        ]))\n",
    "        self.latentDenseLayer = nn.Sequential(OrderedDict([\n",
    "            ('fc2', nn.Linear(2*self.z_dim, 4*fc_dim)),\n",
    "            ('relu5', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalDenseLayer = nn.Sequential(OrderedDict([\n",
    "            ('fc3', nn.Linear(8*fc_dim, fc_dim)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(p=0.5)),\n",
    "            ('fc4', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu7', nn.ReLU()),\n",
    "            ('dropout2', nn.Dropout(p=0.5)),\n",
    "            ('fc5', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu8', nn.ReLU()),\n",
    "            ('dropout3', nn.Dropout(p=0.5)),\n",
    "            ('fc6', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu9', nn.ReLU()),\n",
    "            ('dropout4', nn.Dropout(p=0.5)),\n",
    "            ('fc7', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu10', nn.ReLU()),\n",
    "            ('fc8', nn.Linear(fc_dim, 1))\n",
    "        ]))\n",
    "    \n",
    "    def image_representation(self, x):\n",
    "        inputs_img = torch.reshape(x, [-1, 1, self.img_res, self.img_res])\n",
    "        img_dense_out = self.imageConvLayer(inputs_img)\n",
    "        \n",
    "    def forward(self, z1, z2, img_dense_out):        \n",
    "        inputs_lat = torch.cat((z1, z2), dim=1)\n",
    "        lat_dense_out = self.latentDenseLayer(inputs_lat)\n",
    "        \n",
    "        inputs_final = torch.cat((lat_dense_out, img_dense_out), dim=1)\n",
    "        collision_prediction = self.finalDenseLayer(inputs_final)\n",
    "        \n",
    "        return collision_prediction\n",
    "    \n",
    "    def compute_loss(self, labels, logits):\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels, reduction='sum')\n",
    "    \n",
    "#     def train(self, loss, lr=1e-4):\n",
    "#         optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "torch.FloatTensor\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "enc_dyn_net = AutoEncoder_Dynamics()\n",
    "ed_params = list(enc_dyn_net.parameters())\n",
    "print (len(ed_params))\n",
    "print(ed_params[0].data.type())\n",
    "\n",
    "cc_net = CollisionChecker()\n",
    "c_params = list(cc_net.parameters())\n",
    "print (len(c_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "torch.FloatTensor: torch.Size([8, 1, 5, 5])\n",
      "torch.FloatTensor: torch.Size([8])\n",
      "torch.FloatTensor: torch.Size([8, 8, 5, 5])\n",
      "torch.FloatTensor: torch.Size([8])\n",
      "torch.FloatTensor: torch.Size([8, 8, 5, 5])\n",
      "torch.FloatTensor: torch.Size([8])\n",
      "torch.FloatTensor: torch.Size([1])\n",
      "torch.FloatTensor: torch.Size([256, 16])\n",
      "torch.FloatTensor: torch.Size([256])\n",
      "torch.FloatTensor: torch.Size([256, 256])\n",
      "torch.FloatTensor: torch.Size([256])\n",
      "torch.FloatTensor: torch.Size([2, 256])\n",
      "torch.FloatTensor: torch.Size([2])\n",
      "torch.FloatTensor: torch.Size([128, 4])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([2, 128])\n",
      "torch.FloatTensor: torch.Size([2])\n",
      "torch.FloatTensor: torch.Size([512, 2])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([4, 1, 5, 5])\n",
      "torch.FloatTensor: torch.Size([4])\n",
      "torch.FloatTensor: torch.Size([512, 4096])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([1024, 1024])\n",
      "torch.FloatTensor: torch.Size([1024])\n",
      "Parameters:\n",
      "torch.FloatTensor: torch.Size([10, 1, 7, 7])\n",
      "torch.FloatTensor: torch.Size([10])\n",
      "torch.FloatTensor: torch.Size([10, 10, 7, 7])\n",
      "torch.FloatTensor: torch.Size([10])\n",
      "torch.FloatTensor: torch.Size([10, 10, 7, 7])\n",
      "torch.FloatTensor: torch.Size([10])\n",
      "torch.FloatTensor: torch.Size([512, 10240])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 4])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([128, 1024])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([1, 128])\n",
      "torch.FloatTensor: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for params in [ed_params, c_params]:\n",
    "    print ('Parameters:')\n",
    "    for i in range(len(params)):\n",
    "        print ('{}: {}'.format(params[i].type(), params[i].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "class NumpyCsvDataSet(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.data = np.loadtxt(csv_file, delimiter=',', dtype=float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_training = NumpyCsvDataSet('dynamics_train.csv')\n",
    "dynamics_test = NumpyCsvDataSet('dynamics_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_training_loader = DataLoader(dynamics_training, batch_size=50, shuffle=True)\n",
    "dyn_test_loader = DataLoader(dynamics_training, batch_size=25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 32*32\n",
    "u_dim = 2\n",
    "img_res = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 747.163\n",
      "[1,   200] loss: 548.190\n",
      "[2,   100] loss: 473.249\n",
      "[2,   200] loss: 405.209\n",
      "[3,   100] loss: 359.356\n",
      "[3,   200] loss: 336.032\n",
      "[4,   100] loss: 320.862\n",
      "[4,   200] loss: 306.873\n",
      "[5,   100] loss: 299.660\n",
      "[5,   200] loss: 290.180\n",
      "[6,   100] loss: 283.324\n",
      "[6,   200] loss: 278.987\n",
      "[7,   100] loss: 270.334\n",
      "[7,   200] loss: 265.288\n",
      "[8,   100] loss: 260.970\n",
      "[8,   200] loss: 257.811\n",
      "[9,   100] loss: 253.402\n",
      "[9,   200] loss: 252.980\n",
      "[10,   100] loss: 249.765\n",
      "[10,   200] loss: 246.015\n",
      "[11,   100] loss: 704.441\n",
      "[11,   200] loss: 289.869\n",
      "[12,   100] loss: 271.012\n",
      "[12,   200] loss: 258.622\n",
      "[13,   100] loss: 251.041\n",
      "[13,   200] loss: 248.253\n",
      "[14,   100] loss: 242.734\n",
      "[14,   200] loss: 240.528\n",
      "[15,   100] loss: 237.555\n",
      "[15,   200] loss: 233.871\n",
      "[16,   100] loss: 230.691\n",
      "[16,   200] loss: 230.815\n",
      "[17,   100] loss: 227.161\n",
      "[17,   200] loss: 224.778\n",
      "[18,   100] loss: 222.545\n",
      "[18,   200] loss: 221.161\n",
      "[19,   100] loss: 217.950\n",
      "[19,   200] loss: 218.700\n",
      "[20,   100] loss: 216.254\n",
      "[20,   200] loss: 213.400\n",
      "[21,   100] loss: 213.565\n",
      "[21,   200] loss: 211.858\n",
      "[22,   100] loss: 209.759\n",
      "[22,   200] loss: 208.903\n",
      "[23,   100] loss: 207.625\n",
      "[23,   200] loss: 206.168\n",
      "[24,   100] loss: 205.195\n",
      "[24,   200] loss: 203.850\n",
      "[25,   100] loss: 203.363\n",
      "[25,   200] loss: 199.679\n",
      "[26,   100] loss: 199.612\n",
      "[26,   200] loss: 199.531\n",
      "[27,   100] loss: 197.022\n",
      "[27,   200] loss: 196.655\n",
      "[28,   100] loss: 194.003\n",
      "[28,   200] loss: 196.026\n",
      "[29,   100] loss: 193.368\n",
      "[29,   200] loss: 192.276\n",
      "[30,   100] loss: 191.836\n",
      "[30,   200] loss: 191.377\n",
      "[31,   100] loss: 187.516\n",
      "[31,   200] loss: 190.101\n",
      "[32,   100] loss: 187.564\n",
      "[32,   200] loss: 185.276\n",
      "[33,   100] loss: 185.794\n",
      "[33,   200] loss: 184.605\n",
      "[34,   100] loss: 183.930\n",
      "[34,   200] loss: 182.677\n",
      "[35,   100] loss: 181.781\n",
      "[35,   200] loss: 181.070\n",
      "[36,   100] loss: 180.928\n",
      "[36,   200] loss: 179.472\n",
      "[37,   100] loss: 178.640\n",
      "[37,   200] loss: 177.995\n",
      "[38,   100] loss: 176.306\n",
      "[38,   200] loss: 177.275\n",
      "[39,   100] loss: 174.821\n",
      "[39,   200] loss: 174.806\n",
      "[40,   100] loss: 173.092\n",
      "[40,   200] loss: 174.033\n",
      "[41,   100] loss: 172.759\n",
      "[41,   200] loss: 171.027\n",
      "[42,   100] loss: 171.630\n",
      "[42,   200] loss: 170.431\n",
      "[43,   100] loss: 169.064\n",
      "[43,   200] loss: 169.464\n",
      "[44,   100] loss: 168.215\n",
      "[44,   200] loss: 167.721\n",
      "[45,   100] loss: 167.204\n",
      "[45,   200] loss: 166.725\n",
      "[46,   100] loss: 165.995\n",
      "[46,   200] loss: 165.338\n",
      "[47,   100] loss: 164.294\n",
      "[47,   200] loss: 164.600\n",
      "[48,   100] loss: 163.925\n",
      "[48,   200] loss: 162.078\n",
      "[49,   100] loss: 161.931\n",
      "[49,   200] loss: 161.863\n",
      "[50,   100] loss: 161.643\n",
      "[50,   200] loss: 159.850\n",
      "[51,   100] loss: 160.213\n",
      "[51,   200] loss: 159.591\n",
      "[52,   100] loss: 158.723\n",
      "[52,   200] loss: 158.506\n",
      "[53,   100] loss: 156.548\n",
      "[53,   200] loss: 158.808\n",
      "[54,   100] loss: 156.103\n",
      "[54,   200] loss: 156.595\n",
      "[55,   100] loss: 155.085\n",
      "[55,   200] loss: 155.727\n",
      "[56,   100] loss: 155.231\n",
      "[56,   200] loss: 153.444\n",
      "[57,   100] loss: 153.008\n",
      "[57,   200] loss: 154.378\n",
      "[58,   100] loss: 152.576\n",
      "[58,   200] loss: 152.530\n",
      "[59,   100] loss: 152.755\n",
      "[59,   200] loss: 151.983\n",
      "[60,   100] loss: 150.415\n",
      "[60,   200] loss: 151.205\n",
      "[61,   100] loss: 149.426\n",
      "[61,   200] loss: 150.023\n",
      "[62,   100] loss: 149.209\n",
      "[62,   200] loss: 149.010\n",
      "[63,   100] loss: 148.326\n",
      "[63,   200] loss: 148.015\n",
      "[64,   100] loss: 147.814\n",
      "[64,   200] loss: 147.040\n",
      "[65,   100] loss: 146.478\n",
      "[65,   200] loss: 146.846\n",
      "[66,   100] loss: 145.672\n",
      "[66,   200] loss: 145.714\n",
      "[67,   100] loss: 144.740\n",
      "[67,   200] loss: 145.323\n",
      "[68,   100] loss: 143.808\n",
      "[68,   200] loss: 144.752\n",
      "[69,   100] loss: 142.747\n",
      "[69,   200] loss: 143.506\n",
      "[70,   100] loss: 142.027\n",
      "[70,   200] loss: 142.197\n",
      "[71,   100] loss: 141.537\n",
      "[71,   200] loss: 141.868\n",
      "[72,   100] loss: 142.360\n",
      "[72,   200] loss: 141.288\n",
      "[73,   100] loss: 141.259\n",
      "[73,   200] loss: 140.047\n",
      "[74,   100] loss: 139.671\n",
      "[74,   200] loss: 139.593\n",
      "[75,   100] loss: 139.418\n",
      "[75,   200] loss: 137.835\n",
      "[76,   100] loss: 138.481\n",
      "[76,   200] loss: 138.927\n",
      "[77,   100] loss: 137.537\n",
      "[77,   200] loss: 138.303\n",
      "[78,   100] loss: 136.987\n",
      "[78,   200] loss: 137.092\n",
      "[79,   100] loss: 135.985\n",
      "[79,   200] loss: 137.509\n",
      "[80,   100] loss: 135.767\n",
      "[80,   200] loss: 135.737\n",
      "[81,   100] loss: 135.799\n",
      "[81,   200] loss: 136.123\n",
      "[82,   100] loss: 134.349\n",
      "[82,   200] loss: 135.751\n",
      "[83,   100] loss: 133.670\n",
      "[83,   200] loss: 134.689\n",
      "[84,   100] loss: 133.584\n",
      "[84,   200] loss: 133.981\n",
      "[85,   100] loss: 132.861\n",
      "[85,   200] loss: 133.443\n",
      "[86,   100] loss: 132.484\n",
      "[86,   200] loss: 132.949\n",
      "[87,   100] loss: 132.753\n",
      "[87,   200] loss: 131.764\n",
      "[88,   100] loss: 130.846\n",
      "[88,   200] loss: 132.576\n",
      "[89,   100] loss: 130.510\n",
      "[89,   200] loss: 131.707\n",
      "[90,   100] loss: 130.825\n",
      "[90,   200] loss: 130.575\n",
      "[91,   100] loss: 130.368\n",
      "[91,   200] loss: 130.743\n",
      "[92,   100] loss: 129.815\n",
      "[92,   200] loss: 130.101\n",
      "[93,   100] loss: 129.103\n",
      "[93,   200] loss: 129.552\n",
      "[94,   100] loss: 128.750\n",
      "[94,   200] loss: 128.891\n",
      "[95,   100] loss: 127.881\n",
      "[95,   200] loss: 129.378\n",
      "[96,   100] loss: 127.726\n",
      "[96,   200] loss: 128.924\n",
      "[97,   100] loss: 127.231\n",
      "[97,   200] loss: 127.473\n",
      "[98,   100] loss: 127.252\n",
      "[98,   200] loss: 127.387\n",
      "[99,   100] loss: 126.112\n",
      "[99,   200] loss: 127.585\n",
      "[100,   100] loss: 127.240\n",
      "[100,   200] loss: 126.684\n"
     ]
    }
   ],
   "source": [
    "## Training auto encoder and dynamics network\n",
    "epochs = 100\n",
    "\n",
    "enc_dyn_net = AutoEncoder_Dynamics()\n",
    "enc_dyn_net.train()\n",
    "optimizer = optim.Adam(enc_dyn_net.parameters(), lr=1e-4)\n",
    "\n",
    "expt_prefix = 'AutoEncoderDynamics-Training-'\n",
    "expt_name = expt_prefix + time.strftime('%m-%d-%H-%M-%S')\n",
    "writer = SummaryWriter('runs/' + expt_name)\n",
    "\n",
    "running_loss = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(dyn_training_loader, 0):\n",
    "        data = data.float()\n",
    "        x_t = data[:, :x_dim]\n",
    "        x_tplus = data[:, x_dim:2*x_dim]\n",
    "        x_empty = data[:, 2*x_dim:3*x_dim]\n",
    "        u_t = data[:, 3*x_dim:]\n",
    "        u_t.requires_grad_()\n",
    "        \n",
    "#         if (epoch==0 and i==0):\n",
    "#             writer.add_graph(enc_dyn_net, (x_t, x_tplus, x_empty, u_t))\n",
    "            \n",
    "        x_full, z_t, z_tplus, x_hat_full, z_hat_tplus = enc_dyn_net(x_t, x_tplus, x_empty, u_t)\n",
    "        l2_weight = 1.0 if epoch < 10 else 0.0 # Can use a more sophisticated L2_weight formulation\n",
    "        total_loss, predict_loss_G, predict_loss_L2, recon_loss = enc_dyn_net.compute_loss(u_t, x_full, z_t, z_tplus, x_hat_full, z_hat_tplus, l2_weight)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += np.array([total_loss.item(), predict_loss_G.item(), predict_loss_L2.item(), recon_loss.item()])\n",
    "        if i % 100 == 99:\n",
    "            avg_loss = running_loss / 100\n",
    "            print ('[%d, %5d] loss: %.3f' % (epoch+1, i+1, avg_loss[0]))\n",
    "            index = epoch * len(dyn_training_loader) + i\n",
    "            writer.add_scalar('training_loss', avg_loss[0], index)\n",
    "            writer.add_scalar('predict_loss_G', avg_loss[1], index)\n",
    "            writer.add_scalar('predict_loss_L2', avg_loss[2], index)\n",
    "            writer.add_scalar('recon_loss', avg_loss[3], index)\n",
    "            running_loss[:] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model/autoenc_dyn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_dyn_net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_net = AutoEncoder_Dynamics()\n",
    "test_net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_img = np.random.randint(len(dynamics_test))\n",
    "    data = torch.as_tensor(dynamics_test[test_img].reshape(1,-1))\n",
    "    data = data.float()\n",
    "    x_t = data[:,:x_dim]\n",
    "    x_tplus = data[:,x_dim:2*x_dim]\n",
    "    x_empty = data[:,2*x_dim:3*x_dim]\n",
    "    u_t = data[:,3*x_dim:]\n",
    "\n",
    "    x_full, z_t, z_tplus, x_hat_full, z_hat_tplus = test_net(x_t, x_tplus, x_empty, u_t)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(2,2,1)\n",
    "    plt.imshow(x_t.reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_t')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax = plt.subplot(2,2,2)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.imshow(x_hat_full[0,:].reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_hat_t')\n",
    "    ax = plt.subplot(2,2,3)\n",
    "    plt.imshow(x_tplus.reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_tplus')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax = plt.subplot(2,2,4)\n",
    "    plt.imshow(x_hat_full[1,:].reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_hat_tplus')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(821,)\n",
      "(512,)\n",
      "(351,)\n",
      "(186,)\n",
      "69.818405\n"
     ]
    }
   ],
   "source": [
    "opt = np.get_printoptions()\n",
    "np.set_printoptions(threshold=np.inf, formatter={'float':lambda x: str(x)})\n",
    "expected = x_t.numpy()\n",
    "actual = x_hat_full[0,:].numpy()\n",
    "diff = np.abs(actual - expected)\n",
    "\n",
    "# Q: Explore the input\n",
    "# print (expected)\n",
    "# print ((expected > 30).reshape(32,32))\n",
    "\n",
    "# Q: How varied are the differences?\n",
    "print (diff[diff > 2].shape)\n",
    "print (diff[diff > 10].shape)\n",
    "print (diff[diff > 20].shape)\n",
    "print (diff[diff > 30].shape)\n",
    "print (np.max(diff))\n",
    "\n",
    "# Q: Is there a high diff at obstacles / robot locations and not so high at empty space?\n",
    "# A: Doesn't seem to have such a pattern, so nothing conclusive.\n",
    "# print (diff[expected > 30])\n",
    "\n",
    "# Q: Is the diff uniformly proportional to input?\n",
    "# A: Doesn't seem so either...\n",
    "proportion = diff / expected\n",
    "# print (proportion)\n",
    "# print (proportion[proportion >= 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6494027e-05 -1.8525869e-05]]\n",
      "[[6.1802566e-06 -3.4347177e-06]]\n",
      "[[0.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print (z_t.numpy())\n",
    "print (z_tplus.numpy())\n",
    "print (z_hat_tplus.numpy())\n",
    "np.set_printoptions(**opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to use some z_t and a near by value to decode and observe the full states for near by latent states\n",
    "# However, one problem is decoder network expects an empty image of the full state as input.\n",
    "some_z_t = np.array([[0.5, 0.5]])\n",
    "near_by_z_t = some_z_t + np.random.rand(*some_z_t.shape) * (10**-2)\n",
    "print (near_by_z_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2RRT - A lot of this code (except the tensorflow specific code) is taken from source!\n",
    "plotOn = True\n",
    "num = 1000\n",
    "success = False\n",
    "count_success = 0\n",
    "max_success = 100\n",
    "T = 20\n",
    "connection_radius = 0.15\n",
    "stepsize = 0.1\n",
    "radius_goal = 0.1\n",
    "goal_bias = 0.1\n",
    "cc_cutoff = 0.9 # only accept edges X likely to be collision free\n",
    "\n",
    "# initialize empty tree\n",
    "parents_rrt = np.zeros(num, dtype=int)-1 # index of parent node to each node\n",
    "zs_rrt = np.zeros((num, z_dim)) # positions in latent space of elements of the tree\n",
    "costs_rrt = np.zeros(num) # cost of each sample\n",
    "trajs_rrt = np.zeros((num, T+1, z_dim))\n",
    "T_rrt = np.zeros(num, dtype=int)\n",
    "us_rrt = np.zeros((num, u_dim))\n",
    "\n",
    "if plotOn:\n",
    "    fig1 = plt.figure(figsize=(14,10), dpi=200)\n",
    "plotOn and plt.scatter(samples_rrt[:,0], samples_rrt[:,1], color=\"blue\", s=30, alpha=0.01) # current\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# choose problem\n",
    "idx_problem = randint(0,num_problems-1) \n",
    "x_init_rrt = x_init_problem[idx_problem,:]\n",
    "x_goal_rrt = x_goal_problem[idx_problem,:]\n",
    "xempty_rrt = xempty_problem[idx_problem,:]\n",
    "\n",
    "# In original, x_empty and u are also passed to this call but I don't see a need to do that, validate by running...\n",
    "_, zs_local = enc_dyn_net.encode(x_t=x_init_rrt, x_tplus=x_goal_rrt)\n",
    "\n",
    "zs_rrt[0,:] = zs_local[0,:]\n",
    "z_goal = zs_local[1,:]\n",
    "\n",
    "dummy_input_for_goal = np.zeros((1,u_dim))\n",
    "G_inv_goal = enc_dyn_net.compute_grammian(z_goal, dummy_input_for_goal, self.predict_dynamics(z_goal, dummy_input_for_goal))\n",
    "dense_CC_conv_out = cc_net.image_representation(xempty_rrt)\n",
    "\n",
    "itrs_rrt = 1\n",
    "\n",
    "# explore\n",
    "for i in range(0,num-1):\n",
    "    idx_expand = randint(0,num-1)\n",
    "    sample_expand = samples_rrt[idx_expand,:]\n",
    "    G_inv_expand = G_inv_rrt[idx_expand,:,:]\n",
    "    \n",
    "    if random() < goal_bias:\n",
    "        sample_expand = z_goal\n",
    "        G_inv_expand = G_inv_goal\n",
    "  \n",
    "    # best near neighbor within ball radius\n",
    "    neighbors_heap = []\n",
    "    for nn in range(0,itrs_rrt):\n",
    "        dz = zs_rrt[nn,:] - sample_expand\n",
    "        if dz.dot(G_inv_expand).dot(dz) < connection_radius:\n",
    "            heappush(neighbors_heap,(costs_rrt[nn], nn)) # push 0 in\n",
    "    \n",
    "    if len(neighbors_heap) > 0:\n",
    "        neighbor_entry = heappop(neighbors_heap)\n",
    "        idx_neighbor = neighbor_entry[1]\n",
    "    else: # take the nearest node    \n",
    "        idx_neighbor = -1\n",
    "        neighbor_cost = np.infty\n",
    "        for nn in range(0,itrs_rrt):\n",
    "            dz = zs_rrt[nn,:] - sample_expand\n",
    "            if dz.dot(G_inv_expand).dot(dz) < neighbor_cost:\n",
    "                neighbor_cost = dz.dot(G_inv_expand).dot(dz)\n",
    "                idx_neighbor = nn\n",
    "    \n",
    "    z_expand = zs_rrt[idx_neighbor];\n",
    "    z_expand_idx = idx_neighbor;\n",
    "    \n",
    "    # sample controls and forward propagate\n",
    "    # The may be accelerated by sampling many points in parallel. By then batching the tensorflow call, this shouldn't incur much slowdown\n",
    "    isFree_expand = True\n",
    "    uc_expand = np.random.uniform(-stepsize/5,stepsize/5,size=(z_dim))\n",
    "    T_expand = randint(1,T)\n",
    "    traj_exp = np.zeros((T+1, z_dim))\n",
    "    traj_exp[0,:] = z_expand\n",
    "    \n",
    "    for t in range(0,T_expand):\n",
    "        zp_expand = enc_dyn_net.predict_dynamics(z_expand, uc_expand)\n",
    "        z_expand = zp_expand[0]\n",
    "        traj_exp[t+1,:] = z_expand\n",
    "    \n",
    "    # check collision\n",
    "    y_CC_expand = cc_net.forward(traj_exp[0:T_expand], traj_exp[1:T_expand+1], np.tile(dense_CC_conv_out[0],(T_expand,1)))\n",
    "    \n",
    "    value_CC_expand = 1 / (1 + np.exp(-y_CC_expand))\n",
    "    isNotFree_expand_t = value_CC_expand < cc_cutoff\n",
    "    if np.any(isNotFree_expand_t):\n",
    "        isFree_expand = False\n",
    "        plotOn and plt.scatter(traj_exp[np.where(isNotFree_expand_t)[0][0],0], \n",
    "                               traj_exp[np.where(isNotFree_expand_t)[0][0],1], color=\"black\", s=50, alpha=1)\n",
    "\n",
    "    # can also add the expanded edge up to the state of collision\n",
    "    if not isFree_expand: # the connection wasn't successful\n",
    "        continue;\n",
    "        \n",
    "    # add to tree\n",
    "    costs_rrt[itrs_rrt] = costs_rrt[z_expand_idx] + np.linalg.norm(uc_expand)*T_expand # T_expand + \n",
    "    parents_rrt[itrs_rrt] = z_expand_idx\n",
    "    zs_rrt[itrs_rrt] = zp_expand\n",
    "    trajs_rrt[itrs_rrt] = traj_exp\n",
    "    us_rrt[itrs_rrt] = uc_expand\n",
    "    T_rrt[itrs_rrt] = T_expand\n",
    "    \n",
    "    # or don't break and keep going and take the best over time\n",
    "    if np.linalg.norm(zs_rrt[itrs_rrt,:] - z_goal) < radius_goal:\n",
    "        if count_success == 0:\n",
    "            print 'success'\n",
    "            success = True\n",
    "        count_success += 1\n",
    "        if count_success > max_success:\n",
    "            break;\n",
    "    \n",
    "    itrs_rrt += 1\n",
    "    if np.mod(i,100) == 0:\n",
    "        print('i = ', i,', t = ', time.time()-start_time)\n",
    "\n",
    "# plot nodes and costs\n",
    "max_cost = np.max(costs_rrt)\n",
    "for i in range(1,itrs_rrt):\n",
    "    color = costs_rrt[i]/max_cost\n",
    "    plotOn and plt.scatter(zs_rrt[i,0], zs_rrt[i,1], c=[color, 0, 1-color], s=60, alpha=0.4)\n",
    "    plotOn and plt.plot(trajs_rrt[i,0:T_rrt[i]+1,0], trajs_rrt[i,0:T_rrt[i]+1,1], c='black', alpha=0.3)\n",
    "    \n",
    "# plot solution trajectory\n",
    "if success:\n",
    "    # shorest time path\n",
    "    best_T = np.infty\n",
    "    idx_soln_T = 0\n",
    "    for i in range(0,itrs_rrt):\n",
    "        if np.linalg.norm(zs_rrt[i,:] - z_goal) < radius_goal:\n",
    "            tmp_T = 0\n",
    "            tmp_idx = i\n",
    "            while not parents_rrt[tmp_idx] == -1:\n",
    "                tmp_T += T_rrt[tmp_idx]\n",
    "                tmp_idx = parents_rrt[tmp_idx]\n",
    "            if tmp_T < best_T:\n",
    "                best_T = tmp_T\n",
    "                idx_soln_T = i\n",
    "            \n",
    "    # best cost path\n",
    "    best_T = np.infty\n",
    "    idx_soln_T = 0\n",
    "    for i in range(0,itrs_rrt):\n",
    "        if np.linalg.norm(zs_rrt[i,:] - z_goal) < radius_goal:\n",
    "            if costs_rrt[i] < best_T:\n",
    "                best_T = costs_rrt[i]\n",
    "                idx_soln_T = i\n",
    "        \n",
    "    idx = idx_soln_T\n",
    "    while not parents_rrt[idx] == -1:\n",
    "        plotOn and plt.plot(trajs_rrt[idx,0:T_rrt[idx]+1,0], trajs_rrt[idx,0:T_rrt[idx]+1,1], c='green', alpha=0.8, linewidth=5)\n",
    "        idx = parents_rrt[idx]\n",
    "else: \n",
    "    print \"failure :(\"\n",
    "\n",
    "plotOn and plt.scatter(zs_rrt[0,0], zs_rrt[0,1], color=\"green\", s=500, alpha=0.8) # plot init\n",
    "plotOn and plt.scatter(z_goal[0], z_goal[1], color=\"red\", s=500, alpha=0.8) # plot init\n",
    "plotOn and plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pk-lsbmp",
   "language": "python",
   "name": "pk-lsbmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
