{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from collections import OrderedDict\n",
    "from spatialsoftmax import SpatialSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-2-81ca285e74de>, line 181)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-81ca285e74de>\"\u001b[0;36m, line \u001b[0;32m181\u001b[0m\n\u001b[0;31m    ('conv1', nn.Conv2d(in_channels=1, out_channels=conv_filters, kernel_size, padding)), # kernel_size different than original\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Things that doesn't yet seem to be supported in pytorch:\n",
    "# padding = same\n",
    "# spatial softmax\n",
    "class AutoEncoder_Dynamics(nn.Module):\n",
    "    def __init__(self, img_res=32, z_dim=2, u_dim=2):\n",
    "        super(AutoEncoder_Dynamics, self).__init__()\n",
    "        \n",
    "        self.img_res = img_res\n",
    "        self.x_dim = img_res*img_res\n",
    "        self.z_dim = z_dim\n",
    "        self.u_dim = u_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('conv3', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('softmax', SpatialSoftmax(img_res, img_res, 8)),\n",
    "            ('fc1', nn.Linear(8*self.x_dim, 256)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(p=0.5)),\n",
    "            ('fc2', nn.Linear(256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(256, self.z_dim))\n",
    "        ]))\n",
    "        self.dynamics = nn.Sequential(OrderedDict([\n",
    "            ('d_fc1', nn.Linear(self.z_dim + self.u_dim, 128)),\n",
    "            ('d_relu1', nn.ReLU()),\n",
    "            ('d_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('d_fc2', nn.Linear(128, 128)),\n",
    "            ('d_relu2', nn.ReLU()),\n",
    "            ('d_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('d_fc3', nn.Linear(128, 128)),\n",
    "            ('d_relu3', nn.ReLU()),\n",
    "            ('d_fc4', nn.Linear(128, self.z_dim)),\n",
    "            ('d_relu4', nn.ReLU())\n",
    "        ]))\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            ('dec_fc1', nn.Linear(self.z_dim, 512)),\n",
    "            ('dec_relu1', nn.ReLU()),\n",
    "            ('dec_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc2', nn.Linear(512, 512)),\n",
    "            ('dec_relu2', nn.ReLU()),\n",
    "            ('dec_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc3', nn.Linear(512, 512)),\n",
    "            ('dec_relu3', nn.ReLU()),\n",
    "            ('dec_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc4', nn.Linear(512, 512)),\n",
    "            ('dec_relu4', nn.ReLU())\n",
    "        ]))\n",
    "        self.environment = nn.Sequential(OrderedDict([\n",
    "            ('env_conv1', nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('env_relu1', nn.ReLU()),\n",
    "            ('env_flat1', nn.Flatten()),\n",
    "            ('env_fc1', nn.Linear(4 * self.x_dim, 512)),\n",
    "            ('env_relu1', nn.ReLU()),\n",
    "            ('env_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('env_fc2', nn.Linear(512, 512)),\n",
    "            ('env_relu2', nn.ReLU()),\n",
    "            ('env_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('env_fc3', nn.Linear(512, 512)),\n",
    "            ('env_relu3', nn.ReLU()),\n",
    "            ('env_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('env_fc4', nn.Linear(512, 512)),\n",
    "            ('env_relu4', nn.ReLU())\n",
    "        ]))\n",
    "        self.last_layer = nn.Linear(512 + 512, self.x_dim)\n",
    "    \n",
    "    def encode(self, x_t, x_tplus):\n",
    "        x_full = torch.cat((x_t, x_tplus), dim=0)\n",
    "        input_enc = torch.reshape(x_full, [-1, 1, self.img_res, self.img_res])\n",
    "        z_full = self.encoder(input_enc)\n",
    "        return x_full, z_full\n",
    "\n",
    "    def predict_dynamics(self, z_full, u_t):\n",
    "        batch_size = u_t.size()[0]\n",
    "        z_t = z_full[:batch_size, :]\n",
    "        input_dyn = torch.cat((z_t, u_t), dim=1) #TODO: Do I have to use torch.identity after concatenation? why/not?\n",
    "        z_hat_tplus = self.dynamics(input_dyn)\n",
    "        return z_hat_tplus\n",
    "        \n",
    "    def compute_grammian(self, z_full, u_t, z_hat_tplus):\n",
    "        batch_size = u_t.size()[0]\n",
    "        z_t = z_full[:batch_size, :]\n",
    "        \n",
    "        z_hat_tplus_zero = z_hat_tplus[:, 0]\n",
    "        z_hat_tplus_one = z_hat_tplus[:, 1]\n",
    "\n",
    "        grad_zh0_zt = autograd.grad(z_hat_tplus_zero, z_t, grad_outputs=torch.ones(z_hat_tplus_zero.size()), retain_graph=True)\n",
    "        grad_zh1_zt = autograd.grad(z_hat_tplus_one, z_t, grad_outputs=torch.ones(z_hat_tplus_one.size()), retain_graph=True)\n",
    "        grad_zh0_ut = autograd.grad(z_hat_tplus_zero, u_t, grad_outputs=torch.ones(z_hat_tplus_zero.size()), retain_graph=True)\n",
    "        grad_zh1_ut = autograd.grad(z_hat_tplus_one, u_t, grad_outputs=torch.ones(z_hat_tplus_one.size()), retain_graph=False)\n",
    "        \n",
    "        A = torch.stack([grad_zh0_zt, grad_zh1_zt], dim=1) # N x D_z_hat x D_z  (D_z_hat = D_z = 2)\n",
    "        B = torch.stack([grad_zh0_ut, grad_zh1_ut], dim=1) # N x D_z_hat x D_c  (D_c = 2)\n",
    "        c = self.__expand_dims(z_hat_tplus) - torch.bmm(A, self.__expand_dims(z_t)) - torch.bmm(B, self.__expand_dims(u_t))\n",
    "        AT = torch.transpose(A, 1, 2) # Preserve the batch dimension 0 and transpose dimentions 1 and 2\n",
    "        BT = torch.transpose(B, 1, 2)\n",
    "        \n",
    "        G = torch.bmm(A, torch.bmm(B, torch.bmm(BT, AT))) # N x D_z x D_z (remember D_z_hat = D_z)\n",
    "        offset_for_invertible = (0.0001 * torch.eye(G.size()[1])).expand_as(G)\n",
    "        with torch.no_grad(): # Is this needed? Probably not...\n",
    "            G_inv = torch.inverse(G + offset_for_invertible) # N x D_z x D_z\n",
    "            \n",
    "        return G_inv\n",
    "        \n",
    "    def forward(self, x_t, x_tplus, x_empty, u_t):\n",
    "        '''\n",
    "        x_t, x_tplus, x_empty must be of shape [N, C*H*W] where, N = batch_size, \n",
    "        C = Channels, H = Height, W = Width of image.\n",
    "        u is of shape [N, D_c] where D_c = Control Dimension.\n",
    "        '''\n",
    "        batch_size = u_t.size()[0]\n",
    "\n",
    "        x_full, z_full = self.encode(x_t, x_tplus)\n",
    "        \n",
    "        z_hat_tplus = self.predict_dynamics(z_full, u_t)\n",
    "        \n",
    "        z_t = z_full[:batch_size, :]\n",
    "        input_dec = torch.cat((z_t, z_hat_tplus), dim=0) #TODO: Again, should I use torch.identity here?\n",
    "        output_dec = self.decoder(input_dec)\n",
    "        \n",
    "        x_empty_full = torch.cat((x_empty, x_empty), dim=0)\n",
    "        input_env = torch.reshape(x_empty_full, [-1, 1, self.img_res, self.img_res]) #TODO: identity?\n",
    "        output_env = self.environment(input_env)\n",
    "        \n",
    "        input_last = torch.cat((output_dec, output_env), dim=1)\n",
    "        x_hat_full = self.last_layer(input_last)\n",
    "        \n",
    "        return x_full, z_full, x_hat_full, z_hat_tplus\n",
    "        \n",
    "    def compute_loss(self, u_t, x_full, z_full, x_hat_full, z_hat_tplus, L2_weight):\n",
    "        '''\n",
    "        From a typical pytorch code principles, perhaps this should be a different class\n",
    "        than the net class itself but that's ok for now.\n",
    "        '''\n",
    "        G_inv = self.compute_grammian(z_full, u_t, z_hat_tplus)\n",
    "\n",
    "        batch_size = u_t.size()[0]\n",
    "        z_tplus = z_full[batch_size:, :]\n",
    "\n",
    "        z_diff = self.__expand_dims(z_hat_tplus) - self.__expand_dims(z_tplus) # N x D_z x 1\n",
    "        z_diff_T = torch.transpose(z_diff, 1, 2) # N x 1 x D_z\n",
    "        \n",
    "        predict_loss_G = torch.sum(torch.abs(torch.bmm(z_diff_T, torch.bmm(G_inv, z_diff)))) # N x 1 before sum, scalar after sum\n",
    "        predict_loss_L2 = F.mse_loss(z_hat_tplus, z_tplus,  reduction='sum')\n",
    "        predict_loss = predict_loss_G * (1 - L2_weight) + predict_loss_L2 * L2_weight\n",
    "        \n",
    "        recon_loss = F.mse_loss(x_hat_full, x_full, reduction='sum')\n",
    "        total_loss = predict_loss + recon_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def train(self, total_loss, lr=1e-4):\n",
    "        '''\n",
    "        From a typical pytorch code principles, perhaps this should be a different class\n",
    "        than the net class itself but that's ok for now.\n",
    "        '''\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    def __expand_dims(input):\n",
    "        return input.unsqueeze_(input.dim())\n",
    "        \n",
    "class CollisionChecker(nn.Module):\n",
    "    def __init__(self, img_res=32, z_dim=2, u_dim=2):\n",
    "        super(CollisionChecker, self).__init__()\n",
    "        self.img_res = img_res\n",
    "        self.x_dim = img_res*img_res\n",
    "        self.z_dim = z_dim\n",
    "        self.u_dim = u_dim\n",
    "        conv_filters = 10\n",
    "        padding=3\n",
    "        kernel_size=7\n",
    "        fc_dim = 128\n",
    "        self.imageConvLayer = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=conv_filters, kernel_size=kernel_size, padding=padding)), # kernel_size different than original\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(conv_filters, conv_filters, kernel_size, padding)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('conv3', nn.Conv2d(conv_filters, conv_filters, kernel_size, padding)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('flat1', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(conv_filters*self.x_dim, 4*fc_dim)),\n",
    "            ('relu4', nn.ReLU())\n",
    "        ]))\n",
    "        self.latentDenseLayer = nn.Sequential(OrderedDict([\n",
    "            ('fc2', nn.Linear(2*self.z_dim, 4*fc_dim)),\n",
    "            ('relu5', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalDenseLayer = nn.Sequential(OrderedDict([\n",
    "            ('fc3', nn.Linear(8*fc_dim, fc_dim)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(p=0.5)),\n",
    "            ('fc4', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu7', nn.ReLU()),\n",
    "            ('dropout2', nn.Dropout(p=0.5)),\n",
    "            ('fc5', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu8', nn.ReLU()),\n",
    "            ('dropout3', nn.Dropout(p=0.5)),\n",
    "            ('fc6', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu9', nn.ReLU()),\n",
    "            ('dropout4', nn.Dropout(p=0.5)),\n",
    "            ('fc7', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu10', nn.ReLU()),\n",
    "            ('fc8', nn.Linear(fc_dim, 1))\n",
    "        ]))\n",
    "    \n",
    "    def image_representation(self, x):\n",
    "        inputs_img = torch.reshape(x, [-1, 1, self.img_res, self.img_res])\n",
    "        img_dense_out = self.imageConvLayer(inputs_img)\n",
    "        \n",
    "    def forward(self, z1, z2, img_dense_out):        \n",
    "        inputs_lat = torch.cat((z1, z2), dim=1)\n",
    "        lat_dense_out = self.latentDenseLayer(inputs_lat)\n",
    "        \n",
    "        inputs_final = torch.cat((lat_dense_out, img_dense_out), dim=1)\n",
    "        collision_prediction = self.finalDenseLayer(inputs_final)\n",
    "        \n",
    "        return collision_prediction\n",
    "    \n",
    "    def compute_loss(self, labels, logits):\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels, reduction='sum')\n",
    "    \n",
    "    def train(self, loss, lr=1e-4):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dyn_net = AutoEncoder_Dynamics()\n",
    "ed_params = list(enc_dyn_net.parameters())\n",
    "print (len(ed_params))\n",
    "\n",
    "cc_net = CollisionChecker()\n",
    "c_params = list(cc_net.parameters())\n",
    "print (len(c_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in [ed_params, c_params]:\n",
    "    print ('Parameters:')\n",
    "    for i in range(len(params)):\n",
    "        print (params[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2RRT - A lot of this code (except the tensorflow specific code) is taken from source!\n",
    "plotOn = True\n",
    "num = 1000\n",
    "success = False\n",
    "count_success = 0\n",
    "max_success = 100\n",
    "T = 20\n",
    "connection_radius = 0.15\n",
    "stepsize = 0.1\n",
    "radius_goal = 0.1\n",
    "goal_bias = 0.1\n",
    "cc_cutoff = 0.9 # only accept edges X likely to be collision free\n",
    "\n",
    "# initialize empty tree\n",
    "parents_rrt = np.zeros(num, dtype=int)-1 # index of parent node to each node\n",
    "zs_rrt = np.zeros((num, z_dim)) # positions in latent space of elements of the tree\n",
    "costs_rrt = np.zeros(num) # cost of each sample\n",
    "trajs_rrt = np.zeros((num, T+1, z_dim))\n",
    "T_rrt = np.zeros(num, dtype=int)\n",
    "us_rrt = np.zeros((num, u_dim))\n",
    "\n",
    "if plotOn:\n",
    "    fig1 = plt.figure(figsize=(14,10), dpi=200)\n",
    "plotOn and plt.scatter(samples_rrt[:,0], samples_rrt[:,1], color=\"blue\", s=30, alpha=0.01) # current\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# choose problem\n",
    "idx_problem = randint(0,num_problems-1) \n",
    "x_init_rrt = x_init_problem[idx_problem,:]\n",
    "x_goal_rrt = x_goal_problem[idx_problem,:]\n",
    "xempty_rrt = xempty_problem[idx_problem,:]\n",
    "\n",
    "# In original, x_empty and u are also passed to this call but I don't see a need to do that, validate by running...\n",
    "_, zs_local = enc_dyn_net.encode(x_t=x_init_rrt, x_tplus=x_goal_rrt)\n",
    "\n",
    "zs_rrt[0,:] = zs_local[0,:]\n",
    "z_goal = zs_local[1,:]\n",
    "\n",
    "dummy_input_for_goal = np.zeros((1,u_dim))\n",
    "G_inv_goal = enc_dyn_net.compute_grammian(z_goal, dummy_input_for_goal, self.predict_dynamics(z_goal, dummy_input_for_goal))\n",
    "dense_CC_conv_out = cc_net.image_representation(xempty_rrt)\n",
    "\n",
    "itrs_rrt = 1\n",
    "\n",
    "# explore\n",
    "for i in range(0,num-1):\n",
    "    idx_expand = randint(0,num-1)\n",
    "    sample_expand = samples_rrt[idx_expand,:]\n",
    "    G_inv_expand = G_inv_rrt[idx_expand,:,:]\n",
    "    \n",
    "    if random() < goal_bias:\n",
    "        sample_expand = z_goal\n",
    "        G_inv_expand = G_inv_goal\n",
    "  \n",
    "    # best near neighbor within ball radius\n",
    "    neighbors_heap = []\n",
    "    for nn in range(0,itrs_rrt):\n",
    "        dz = zs_rrt[nn,:] - sample_expand\n",
    "        if dz.dot(G_inv_expand).dot(dz) < connection_radius:\n",
    "            heappush(neighbors_heap,(costs_rrt[nn], nn)) # push 0 in\n",
    "    \n",
    "    if len(neighbors_heap) > 0:\n",
    "        neighbor_entry = heappop(neighbors_heap)\n",
    "        idx_neighbor = neighbor_entry[1]\n",
    "    else: # take the nearest node    \n",
    "        idx_neighbor = -1\n",
    "        neighbor_cost = np.infty\n",
    "        for nn in range(0,itrs_rrt):\n",
    "            dz = zs_rrt[nn,:] - sample_expand\n",
    "            if dz.dot(G_inv_expand).dot(dz) < neighbor_cost:\n",
    "                neighbor_cost = dz.dot(G_inv_expand).dot(dz)\n",
    "                idx_neighbor = nn\n",
    "    \n",
    "    z_expand = zs_rrt[idx_neighbor];\n",
    "    z_expand_idx = idx_neighbor;\n",
    "    \n",
    "    # sample controls and forward propagate\n",
    "    # The may be accelerated by sampling many points in parallel. By then batching the tensorflow call, this shouldn't incur much slowdown\n",
    "    isFree_expand = True\n",
    "    uc_expand = np.random.uniform(-stepsize/5,stepsize/5,size=(z_dim))\n",
    "    T_expand = randint(1,T)\n",
    "    traj_exp = np.zeros((T+1, z_dim))\n",
    "    traj_exp[0,:] = z_expand\n",
    "    \n",
    "    for t in range(0,T_expand):\n",
    "        zp_expand = enc_dyn_net.predict_dynamics(z_expand, uc_expand)\n",
    "        z_expand = zp_expand[0]\n",
    "        traj_exp[t+1,:] = z_expand\n",
    "    \n",
    "    # check collision\n",
    "    y_CC_expand = cc_net.forward(traj_exp[0:T_expand], traj_exp[1:T_expand+1], np.tile(dense_CC_conv_out[0],(T_expand,1)))\n",
    "    \n",
    "    value_CC_expand = 1 / (1 + np.exp(-y_CC_expand))\n",
    "    isNotFree_expand_t = value_CC_expand < cc_cutoff\n",
    "    if np.any(isNotFree_expand_t):\n",
    "        isFree_expand = False\n",
    "        plotOn and plt.scatter(traj_exp[np.where(isNotFree_expand_t)[0][0],0], \n",
    "                               traj_exp[np.where(isNotFree_expand_t)[0][0],1], color=\"black\", s=50, alpha=1)\n",
    "\n",
    "    # can also add the expanded edge up to the state of collision\n",
    "    if not isFree_expand: # the connection wasn't successful\n",
    "        continue;\n",
    "        \n",
    "    # add to tree\n",
    "    costs_rrt[itrs_rrt] = costs_rrt[z_expand_idx] + np.linalg.norm(uc_expand)*T_expand # T_expand + \n",
    "    parents_rrt[itrs_rrt] = z_expand_idx\n",
    "    zs_rrt[itrs_rrt] = zp_expand\n",
    "    trajs_rrt[itrs_rrt] = traj_exp\n",
    "    us_rrt[itrs_rrt] = uc_expand\n",
    "    T_rrt[itrs_rrt] = T_expand\n",
    "    \n",
    "    # or don't break and keep going and take the best over time\n",
    "    if np.linalg.norm(zs_rrt[itrs_rrt,:] - z_goal) < radius_goal:\n",
    "        if count_success == 0:\n",
    "            print 'success'\n",
    "            success = True\n",
    "        count_success += 1\n",
    "        if count_success > max_success:\n",
    "            break;\n",
    "    \n",
    "    itrs_rrt += 1\n",
    "    if np.mod(i,100) == 0:\n",
    "        print('i = ', i,', t = ', time.time()-start_time)\n",
    "\n",
    "# plot nodes and costs\n",
    "max_cost = np.max(costs_rrt)\n",
    "for i in range(1,itrs_rrt):\n",
    "    color = costs_rrt[i]/max_cost\n",
    "    plotOn and plt.scatter(zs_rrt[i,0], zs_rrt[i,1], c=[color, 0, 1-color], s=60, alpha=0.4)\n",
    "    plotOn and plt.plot(trajs_rrt[i,0:T_rrt[i]+1,0], trajs_rrt[i,0:T_rrt[i]+1,1], c='black', alpha=0.3)\n",
    "    \n",
    "# plot solution trajectory\n",
    "if success:\n",
    "    # shorest time path\n",
    "    best_T = np.infty\n",
    "    idx_soln_T = 0\n",
    "    for i in range(0,itrs_rrt):\n",
    "        if np.linalg.norm(zs_rrt[i,:] - z_goal) < radius_goal:\n",
    "            tmp_T = 0\n",
    "            tmp_idx = i\n",
    "            while not parents_rrt[tmp_idx] == -1:\n",
    "                tmp_T += T_rrt[tmp_idx]\n",
    "                tmp_idx = parents_rrt[tmp_idx]\n",
    "            if tmp_T < best_T:\n",
    "                best_T = tmp_T\n",
    "                idx_soln_T = i\n",
    "            \n",
    "    # best cost path\n",
    "    best_T = np.infty\n",
    "    idx_soln_T = 0\n",
    "    for i in range(0,itrs_rrt):\n",
    "        if np.linalg.norm(zs_rrt[i,:] - z_goal) < radius_goal:\n",
    "            if costs_rrt[i] < best_T:\n",
    "                best_T = costs_rrt[i]\n",
    "                idx_soln_T = i\n",
    "        \n",
    "    idx = idx_soln_T\n",
    "    while not parents_rrt[idx] == -1:\n",
    "        plotOn and plt.plot(trajs_rrt[idx,0:T_rrt[idx]+1,0], trajs_rrt[idx,0:T_rrt[idx]+1,1], c='green', alpha=0.8, linewidth=5)\n",
    "        idx = parents_rrt[idx]\n",
    "else: \n",
    "    print \"failure :(\"\n",
    "\n",
    "plotOn and plt.scatter(zs_rrt[0,0], zs_rrt[0,1], color=\"green\", s=500, alpha=0.8) # plot init\n",
    "plotOn and plt.scatter(z_goal[0], z_goal[1], color=\"red\", s=500, alpha=0.8) # plot init\n",
    "plotOn and plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pk-lsbmp",
   "language": "python",
   "name": "pk-lsbmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
