{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from spatialsoftmax import SpatialSoftmax\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things that doesn't yet seem to be supported in pytorch:\n",
    "# padding = same\n",
    "# spatial softmax\n",
    "class AutoEncoder_Dynamics(nn.Module):\n",
    "    def __init__(self, img_res=32, z_dim=2, u_dim=2):\n",
    "        super(AutoEncoder_Dynamics, self).__init__()\n",
    "        \n",
    "        self.img_res = img_res\n",
    "        self.x_dim = img_res*img_res\n",
    "        self.z_dim = z_dim\n",
    "        self.u_dim = u_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('conv3', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('softmax', SpatialSoftmax(img_res, img_res, 8)),\n",
    "            ('fc1', nn.Linear(8*2, 256)), # Spatial softmax will result in 2 values per channel 1 along height and 1 along width.\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(p=0.5)),\n",
    "            ('fc2', nn.Linear(256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(256, self.z_dim))\n",
    "        ]))\n",
    "#         self.encoder = self.encoder.float()\n",
    "        self.dynamics = nn.Sequential(OrderedDict([\n",
    "            ('d_fc1', nn.Linear(self.z_dim + self.u_dim, 128)),\n",
    "            ('d_relu1', nn.ReLU()),\n",
    "            ('d_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('d_fc2', nn.Linear(128, 128)),\n",
    "            ('d_relu2', nn.ReLU()),\n",
    "            ('d_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('d_fc3', nn.Linear(128, 128)),\n",
    "            ('d_relu3', nn.ReLU()),\n",
    "            ('d_fc4', nn.Linear(128, self.z_dim)),\n",
    "            ('d_relu4', nn.ReLU())\n",
    "        ]))\n",
    "#         self.dynamics = self.dynamics.float()\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            ('dec_fc1', nn.Linear(self.z_dim, 512)),\n",
    "            ('dec_relu1', nn.ReLU()),\n",
    "            ('dec_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc2', nn.Linear(512, 512)),\n",
    "            ('dec_relu2', nn.ReLU()),\n",
    "            ('dec_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc3', nn.Linear(512, 512)),\n",
    "            ('dec_relu3', nn.ReLU()),\n",
    "            ('dec_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc4', nn.Linear(512, 512)),\n",
    "            ('dec_relu4', nn.ReLU())\n",
    "        ]))\n",
    "#         self.decoder = self.decoder.float()\n",
    "        self.environment = nn.Sequential(OrderedDict([\n",
    "            ('env_conv1', nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('env_relu1', nn.ReLU()),\n",
    "            ('env_flat1', nn.Flatten()),\n",
    "            ('env_fc1', nn.Linear(4 * self.x_dim, 512)),\n",
    "            ('env_relu1', nn.ReLU()),\n",
    "            ('env_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('env_fc2', nn.Linear(512, 512)),\n",
    "            ('env_relu2', nn.ReLU()),\n",
    "            ('env_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('env_fc3', nn.Linear(512, 512)),\n",
    "            ('env_relu3', nn.ReLU()),\n",
    "            ('env_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('env_fc4', nn.Linear(512, 512)),\n",
    "            ('env_relu4', nn.ReLU())\n",
    "        ]))\n",
    "#         self.environment = self.environment.float()\n",
    "        self.last_layer = nn.Linear(512 + 512, self.x_dim)\n",
    "#         self.last_layer = self.last_layer.float()\n",
    "    \n",
    "    def encode(self, x_t, x_tplus):\n",
    "        x_full = torch.cat((x_t, x_tplus), dim=0)\n",
    "        input_enc = torch.reshape(x_full, [-1, 1, self.img_res, self.img_res])\n",
    "        z_full = self.encoder(input_enc)\n",
    "        return x_full, z_full\n",
    "\n",
    "    def predict_dynamics(self, z_t, u_t):\n",
    "        input_dyn = torch.cat((z_t, u_t), dim=1) #TODO: Do I have to use torch.identity after concatenation? why/not?\n",
    "        z_hat_tplus = self.dynamics(input_dyn)\n",
    "        return z_hat_tplus\n",
    "        \n",
    "    def compute_grammian(self, z_t, u_t, z_hat_tplus, retain_graph_after_last_grad=True):\n",
    "        z_hat_tplus_zero = z_hat_tplus[:, 0]\n",
    "        z_hat_tplus_one = z_hat_tplus[:, 1]\n",
    "\n",
    "        grad_zh0_zt = autograd.grad(z_hat_tplus_zero, z_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_zero.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=True)[0]\n",
    "        grad_zh1_zt = autograd.grad(z_hat_tplus_one, z_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_one.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=True)[0]\n",
    "        grad_zh0_ut = autograd.grad(z_hat_tplus_zero, u_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_zero.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=True)[0]\n",
    "        grad_zh1_ut = autograd.grad(z_hat_tplus_one, u_t,\n",
    "                                    grad_outputs=torch.ones(z_hat_tplus_one.size()),\n",
    "#                                     allow_unused=True,\n",
    "                                    retain_graph=retain_graph_after_last_grad)[0]\n",
    "\n",
    "        A = torch.stack([grad_zh0_zt, grad_zh1_zt], dim=1) # N x D_z_hat x D_z  (D_z_hat = D_z = 2)\n",
    "        B = torch.stack([grad_zh0_ut, grad_zh1_ut], dim=1) # N x D_z_hat x D_c  (D_c = 2)\n",
    "        c = self.__expand_dims(z_hat_tplus) - torch.bmm(A, self.__expand_dims(z_t)) - torch.bmm(B, self.__expand_dims(u_t))\n",
    "        AT = torch.transpose(A, 1, 2) # Preserve the batch dimension 0 and transpose dimentions 1 and 2\n",
    "        BT = torch.transpose(B, 1, 2)\n",
    "        \n",
    "        G = torch.bmm(A, torch.bmm(B, torch.bmm(BT, AT))) # N x D_z x D_z (remember D_z_hat = D_z)\n",
    "        offset_for_invertible = (0.0001 * torch.eye(G.size()[1])).expand_as(G)\n",
    "        with torch.no_grad(): # Is this needed? Probably not...\n",
    "            G_inv = torch.inverse(G + offset_for_invertible) # N x D_z x D_z\n",
    "            \n",
    "        return G_inv, A, c\n",
    "        \n",
    "    def forward(self, x_t, x_tplus, x_empty, u_t):\n",
    "        '''\n",
    "        x_t, x_tplus, x_empty must be of shape [N, C*H*W] where, N = batch_size, \n",
    "        C = Channels, H = Height, W = Width of image.\n",
    "        u is of shape [N, D_c] where D_c = Control Dimension.\n",
    "        '''\n",
    "        batch_size = u_t.size()[0]\n",
    "\n",
    "        x_full, z_full = self.encode(x_t, x_tplus)\n",
    "        \n",
    "        z_t = z_full[:batch_size, :]\n",
    "        z_tplus = z_full[batch_size:, :]\n",
    "\n",
    "        z_hat_tplus = self.predict_dynamics(z_t, u_t)\n",
    "        \n",
    "        input_dec = torch.cat((z_t, z_hat_tplus), dim=0) #TODO: Again, should I use torch.identity here?\n",
    "        output_dec = self.decoder(input_dec)\n",
    "        \n",
    "        x_empty_full = torch.cat((x_empty, x_empty), dim=0)\n",
    "        input_env = torch.reshape(x_empty_full, [-1, 1, self.img_res, self.img_res]) #TODO: identity?\n",
    "        output_env = self.environment(input_env)\n",
    "        \n",
    "        input_last = torch.cat((output_dec, output_env), dim=1)\n",
    "        x_hat_full = self.last_layer(input_last)\n",
    "        \n",
    "        return x_full, z_t, z_tplus, x_hat_full, z_hat_tplus\n",
    "        \n",
    "    def compute_loss(self, u_t, x_full, z_t, z_tplus, x_hat_full, z_hat_tplus, L2_weight):\n",
    "        '''\n",
    "        From a typical pytorch code principles, perhaps this should be a different class\n",
    "        than the net class itself but that's ok for now.\n",
    "        '''\n",
    "        G_inv, _, _ = self.compute_grammian(z_t, u_t, z_hat_tplus)\n",
    "\n",
    "        z_diff = self.__expand_dims(z_hat_tplus) - self.__expand_dims(z_tplus) # N x D_z x 1\n",
    "        z_diff_T = torch.transpose(z_diff, 1, 2) # N x 1 x D_z\n",
    "        \n",
    "        predict_loss_G = torch.sum(torch.abs(torch.bmm(z_diff_T, torch.bmm(G_inv, z_diff)))) # N x 1 before sum, scalar after sum\n",
    "        predict_loss_L2 = F.mse_loss(z_hat_tplus, z_tplus,  reduction='mean')\n",
    "        predict_loss = predict_loss_G * (1 - L2_weight) + predict_loss_L2 * L2_weight\n",
    "        \n",
    "        recon_loss = F.mse_loss(x_hat_full, x_full, reduction='mean')\n",
    "        total_loss = predict_loss + recon_loss\n",
    "        \n",
    "        return total_loss, predict_loss_G, predict_loss_L2, recon_loss\n",
    "    \n",
    "    def __expand_dims(self, input):\n",
    "        return input.unsqueeze(input.dim())\n",
    "        \n",
    "class CollisionChecker(nn.Module):\n",
    "    def __init__(self, img_res=32, z_dim=2, u_dim=2):\n",
    "        super(CollisionChecker, self).__init__()\n",
    "        self.img_res = img_res\n",
    "        self.x_dim = img_res*img_res\n",
    "        self.z_dim = z_dim\n",
    "        self.u_dim = u_dim\n",
    "        conv_filters = 10\n",
    "        padding=3\n",
    "        kernel_size=7\n",
    "        fc_dim = 128\n",
    "        self.imageConvLayer = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=conv_filters, kernel_size=kernel_size, padding=padding)), # kernel_size different than original\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(conv_filters, conv_filters, kernel_size, padding)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('conv3', nn.Conv2d(conv_filters, conv_filters, kernel_size, padding)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('flat1', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(conv_filters*self.x_dim, 4*fc_dim)),\n",
    "            ('relu4', nn.ReLU())\n",
    "        ]))\n",
    "        self.latentDenseLayer = nn.Sequential(OrderedDict([\n",
    "            ('fc2', nn.Linear(2*self.z_dim, 4*fc_dim)),\n",
    "            ('relu5', nn.ReLU())\n",
    "        ]))\n",
    "        self.finalDenseLayer = nn.Sequential(OrderedDict([\n",
    "            ('fc3', nn.Linear(8*fc_dim, fc_dim)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(p=0.5)),\n",
    "            ('fc4', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu7', nn.ReLU()),\n",
    "            ('dropout2', nn.Dropout(p=0.5)),\n",
    "            ('fc5', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu8', nn.ReLU()),\n",
    "            ('dropout3', nn.Dropout(p=0.5)),\n",
    "            ('fc6', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu9', nn.ReLU()),\n",
    "            ('dropout4', nn.Dropout(p=0.5)),\n",
    "            ('fc7', nn.Linear(fc_dim, fc_dim)),\n",
    "            ('relu10', nn.ReLU()),\n",
    "            ('fc8', nn.Linear(fc_dim, 1))\n",
    "        ]))\n",
    "    \n",
    "    def image_representation(self, x):\n",
    "        inputs_img = torch.reshape(x, [-1, 1, self.img_res, self.img_res])\n",
    "        img_dense_out = self.imageConvLayer(inputs_img)\n",
    "        \n",
    "    def forward(self, z1, z2, img_dense_out):        \n",
    "        inputs_lat = torch.cat((z1, z2), dim=1)\n",
    "        lat_dense_out = self.latentDenseLayer(inputs_lat)\n",
    "        \n",
    "        inputs_final = torch.cat((lat_dense_out, img_dense_out), dim=1)\n",
    "        collision_prediction = self.finalDenseLayer(inputs_final)\n",
    "        \n",
    "        return collision_prediction\n",
    "    \n",
    "    def compute_loss(self, labels, logits):\n",
    "        return F.binary_cross_entropy_with_logits(logits, labels, reduction='sum')\n",
    "    \n",
    "#     def train(self, loss, lr=1e-4):\n",
    "#         optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "torch.FloatTensor\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "enc_dyn_net = AutoEncoder_Dynamics()\n",
    "ed_params = list(enc_dyn_net.parameters())\n",
    "print (len(ed_params))\n",
    "print(ed_params[0].data.type())\n",
    "\n",
    "cc_net = CollisionChecker()\n",
    "c_params = list(cc_net.parameters())\n",
    "print (len(c_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "torch.FloatTensor: torch.Size([8, 1, 5, 5])\n",
      "torch.FloatTensor: torch.Size([8])\n",
      "torch.FloatTensor: torch.Size([8, 8, 5, 5])\n",
      "torch.FloatTensor: torch.Size([8])\n",
      "torch.FloatTensor: torch.Size([8, 8, 5, 5])\n",
      "torch.FloatTensor: torch.Size([8])\n",
      "torch.FloatTensor: torch.Size([1])\n",
      "torch.FloatTensor: torch.Size([256, 16])\n",
      "torch.FloatTensor: torch.Size([256])\n",
      "torch.FloatTensor: torch.Size([256, 256])\n",
      "torch.FloatTensor: torch.Size([256])\n",
      "torch.FloatTensor: torch.Size([2, 256])\n",
      "torch.FloatTensor: torch.Size([2])\n",
      "torch.FloatTensor: torch.Size([128, 4])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([2, 128])\n",
      "torch.FloatTensor: torch.Size([2])\n",
      "torch.FloatTensor: torch.Size([512, 2])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([4, 1, 5, 5])\n",
      "torch.FloatTensor: torch.Size([4])\n",
      "torch.FloatTensor: torch.Size([512, 4096])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 512])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([1024, 1024])\n",
      "torch.FloatTensor: torch.Size([1024])\n",
      "Parameters:\n",
      "torch.FloatTensor: torch.Size([10, 1, 7, 7])\n",
      "torch.FloatTensor: torch.Size([10])\n",
      "torch.FloatTensor: torch.Size([10, 10, 7, 7])\n",
      "torch.FloatTensor: torch.Size([10])\n",
      "torch.FloatTensor: torch.Size([10, 10, 7, 7])\n",
      "torch.FloatTensor: torch.Size([10])\n",
      "torch.FloatTensor: torch.Size([512, 10240])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([512, 4])\n",
      "torch.FloatTensor: torch.Size([512])\n",
      "torch.FloatTensor: torch.Size([128, 1024])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([128, 128])\n",
      "torch.FloatTensor: torch.Size([128])\n",
      "torch.FloatTensor: torch.Size([1, 128])\n",
      "torch.FloatTensor: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for params in [ed_params, c_params]:\n",
    "    print ('Parameters:')\n",
    "    for i in range(len(params)):\n",
    "        print ('{}: {}'.format(params[i].type(), params[i].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "class NumpyCsvDataSet(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.data = np.loadtxt(csv_file, delimiter=',', dtype=float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_training = NumpyCsvDataSet('dynamics_train.csv')\n",
    "dynamics_test = NumpyCsvDataSet('dynamics_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_training_loader = DataLoader(dynamics_training, batch_size=50, shuffle=True)\n",
    "dyn_test_loader = DataLoader(dynamics_training, batch_size=25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 32*32\n",
    "u_dim = 2\n",
    "img_res = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model/autoenc_dyn.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dyn_net = AutoEncoder_Dynamics()\n",
    "enc_dyn_net.train()\n",
    "\n",
    "resume_previous_training = True\n",
    "\n",
    "if resume_previous_training:\n",
    "    enc_dyn_net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 126.352\n",
      "[1,   200] loss: 125.872\n",
      "[2,   100] loss: 125.449\n",
      "[2,   200] loss: 125.186\n",
      "[3,   100] loss: 124.863\n",
      "[3,   200] loss: 124.837\n",
      "[4,   100] loss: 124.623\n",
      "[4,   200] loss: 125.534\n",
      "[5,   100] loss: 124.334\n",
      "[5,   200] loss: 125.092\n",
      "[6,   100] loss: 123.717\n",
      "[6,   200] loss: 124.852\n",
      "[7,   100] loss: 122.965\n",
      "[7,   200] loss: 124.396\n",
      "[8,   100] loss: 124.683\n",
      "[8,   200] loss: 123.674\n",
      "[9,   100] loss: 122.543\n",
      "[9,   200] loss: 123.739\n",
      "[10,   100] loss: 122.041\n",
      "[10,   200] loss: 123.764\n",
      "[11,   100] loss: 126.136\n",
      "[11,   200] loss: 123.791\n",
      "[12,   100] loss: 121.568\n",
      "[12,   200] loss: 122.135\n",
      "[13,   100] loss: 121.575\n",
      "[13,   200] loss: 121.715\n",
      "[14,   100] loss: 122.424\n",
      "[14,   200] loss: 120.783\n",
      "[15,   100] loss: 121.441\n",
      "[15,   200] loss: 121.472\n",
      "[16,   100] loss: 121.046\n",
      "[16,   200] loss: 120.718\n",
      "[17,   100] loss: 120.147\n",
      "[17,   200] loss: 120.349\n",
      "[18,   100] loss: 120.819\n",
      "[18,   200] loss: 120.502\n",
      "[19,   100] loss: 121.086\n",
      "[19,   200] loss: 119.761\n",
      "[20,   100] loss: 120.548\n",
      "[20,   200] loss: 119.096\n",
      "[21,   100] loss: 118.831\n",
      "[21,   200] loss: 119.627\n",
      "[22,   100] loss: 118.736\n",
      "[22,   200] loss: 119.496\n",
      "[23,   100] loss: 118.903\n",
      "[23,   200] loss: 119.300\n",
      "[24,   100] loss: 118.152\n",
      "[24,   200] loss: 118.711\n",
      "[25,   100] loss: 117.602\n",
      "[25,   200] loss: 118.288\n",
      "[26,   100] loss: 118.575\n",
      "[26,   200] loss: 117.988\n",
      "[27,   100] loss: 118.675\n",
      "[27,   200] loss: 117.666\n",
      "[28,   100] loss: 117.981\n",
      "[28,   200] loss: 117.299\n",
      "[29,   100] loss: 117.242\n",
      "[29,   200] loss: 117.780\n",
      "[30,   100] loss: 117.060\n",
      "[30,   200] loss: 117.089\n",
      "[31,   100] loss: 116.524\n",
      "[31,   200] loss: 117.081\n",
      "[32,   100] loss: 115.755\n",
      "[32,   200] loss: 117.124\n",
      "[33,   100] loss: 116.599\n",
      "[33,   200] loss: 116.219\n",
      "[34,   100] loss: 116.034\n",
      "[34,   200] loss: 116.573\n",
      "[35,   100] loss: 115.644\n",
      "[35,   200] loss: 115.651\n",
      "[36,   100] loss: 115.468\n",
      "[36,   200] loss: 116.300\n",
      "[37,   100] loss: 115.241\n",
      "[37,   200] loss: 115.560\n",
      "[38,   100] loss: 115.112\n",
      "[38,   200] loss: 116.037\n",
      "[39,   100] loss: 115.068\n",
      "[39,   200] loss: 115.341\n",
      "[40,   100] loss: 115.296\n",
      "[40,   200] loss: 114.384\n",
      "[41,   100] loss: 114.649\n",
      "[41,   200] loss: 114.881\n",
      "[42,   100] loss: 114.779\n",
      "[42,   200] loss: 114.660\n",
      "[43,   100] loss: 113.729\n",
      "[43,   200] loss: 114.209\n",
      "[44,   100] loss: 114.235\n",
      "[44,   200] loss: 114.161\n",
      "[45,   100] loss: 113.675\n",
      "[45,   200] loss: 114.570\n",
      "[46,   100] loss: 113.300\n",
      "[46,   200] loss: 113.730\n",
      "[47,   100] loss: 114.080\n",
      "[47,   200] loss: 112.997\n",
      "[48,   100] loss: 113.268\n",
      "[48,   200] loss: 113.528\n",
      "[49,   100] loss: 112.978\n",
      "[49,   200] loss: 112.684\n",
      "[50,   100] loss: 112.215\n",
      "[50,   200] loss: 112.989\n",
      "[51,   100] loss: 112.424\n",
      "[51,   200] loss: 112.984\n",
      "[52,   100] loss: 112.362\n",
      "[52,   200] loss: 112.587\n",
      "[53,   100] loss: 112.210\n",
      "[53,   200] loss: 112.037\n",
      "[54,   100] loss: 112.127\n",
      "[54,   200] loss: 111.446\n",
      "[55,   100] loss: 111.767\n",
      "[55,   200] loss: 111.966\n",
      "[56,   100] loss: 111.101\n",
      "[56,   200] loss: 112.290\n",
      "[57,   100] loss: 111.478\n",
      "[57,   200] loss: 112.007\n",
      "[58,   100] loss: 110.548\n",
      "[58,   200] loss: 111.204\n",
      "[59,   100] loss: 110.224\n",
      "[59,   200] loss: 111.863\n",
      "[60,   100] loss: 111.109\n",
      "[60,   200] loss: 110.805\n",
      "[61,   100] loss: 110.884\n",
      "[61,   200] loss: 110.091\n",
      "[62,   100] loss: 110.011\n",
      "[62,   200] loss: 110.897\n",
      "[63,   100] loss: 109.821\n",
      "[63,   200] loss: 110.492\n",
      "[64,   100] loss: 110.269\n",
      "[64,   200] loss: 110.223\n",
      "[65,   100] loss: 109.899\n",
      "[65,   200] loss: 109.814\n",
      "[66,   100] loss: 110.208\n",
      "[66,   200] loss: 109.480\n",
      "[67,   100] loss: 110.251\n",
      "[67,   200] loss: 109.394\n",
      "[68,   100] loss: 109.812\n",
      "[68,   200] loss: 109.331\n",
      "[69,   100] loss: 108.717\n",
      "[69,   200] loss: 109.599\n",
      "[70,   100] loss: 108.792\n",
      "[70,   200] loss: 109.344\n",
      "[71,   100] loss: 107.848\n",
      "[71,   200] loss: 109.471\n",
      "[72,   100] loss: 108.333\n",
      "[72,   200] loss: 109.207\n",
      "[73,   100] loss: 108.992\n",
      "[73,   200] loss: 109.045\n",
      "[74,   100] loss: 108.263\n",
      "[74,   200] loss: 108.609\n",
      "[75,   100] loss: 108.588\n",
      "[75,   200] loss: 108.681\n",
      "[76,   100] loss: 107.865\n",
      "[76,   200] loss: 108.646\n",
      "[77,   100] loss: 107.478\n",
      "[77,   200] loss: 108.472\n",
      "[78,   100] loss: 107.900\n",
      "[78,   200] loss: 108.069\n",
      "[79,   100] loss: 107.817\n",
      "[79,   200] loss: 107.934\n",
      "[80,   100] loss: 107.844\n",
      "[80,   200] loss: 107.933\n",
      "[81,   100] loss: 107.898\n",
      "[81,   200] loss: 107.096\n",
      "[82,   100] loss: 107.944\n",
      "[82,   200] loss: 107.400\n",
      "[83,   100] loss: 107.064\n",
      "[83,   200] loss: 108.251\n",
      "[84,   100] loss: 106.547\n",
      "[84,   200] loss: 107.154\n",
      "[85,   100] loss: 107.120\n",
      "[85,   200] loss: 107.305\n",
      "[86,   100] loss: 106.998\n",
      "[86,   200] loss: 106.378\n",
      "[87,   100] loss: 106.568\n",
      "[87,   200] loss: 106.695\n",
      "[88,   100] loss: 106.527\n",
      "[88,   200] loss: 106.712\n",
      "[89,   100] loss: 106.184\n",
      "[89,   200] loss: 106.898\n",
      "[90,   100] loss: 106.176\n",
      "[90,   200] loss: 106.078\n",
      "[91,   100] loss: 106.349\n",
      "[91,   200] loss: 106.594\n",
      "[92,   100] loss: 106.328\n",
      "[92,   200] loss: 106.268\n",
      "[93,   100] loss: 106.165\n",
      "[93,   200] loss: 105.150\n",
      "[94,   100] loss: 106.036\n",
      "[94,   200] loss: 105.531\n",
      "[95,   100] loss: 105.869\n",
      "[95,   200] loss: 105.666\n",
      "[96,   100] loss: 104.953\n",
      "[96,   200] loss: 105.643\n",
      "[97,   100] loss: 104.838\n",
      "[97,   200] loss: 105.759\n",
      "[98,   100] loss: 105.279\n",
      "[98,   200] loss: 105.489\n",
      "[99,   100] loss: 105.335\n",
      "[99,   200] loss: 105.039\n",
      "[100,   100] loss: 104.407\n",
      "[100,   200] loss: 105.928\n"
     ]
    }
   ],
   "source": [
    "## Training auto encoder and dynamics network\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.Adam(enc_dyn_net.parameters(), lr=1e-4)\n",
    "\n",
    "expt_prefix = 'AutoEncoderDynamics-Training-'\n",
    "if resume_previous_training:\n",
    "    expt_prefix = expt_prefix + 'ResumedAt-'\n",
    "expt_name = expt_prefix + time.strftime('%m-%d-%H-%M-%S')\n",
    "writer = SummaryWriter('runs/' + expt_name)\n",
    "\n",
    "running_loss = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(dyn_training_loader, 0):\n",
    "        data = data.float()\n",
    "        x_t = data[:, :x_dim]\n",
    "        x_tplus = data[:, x_dim:2*x_dim]\n",
    "        x_empty = data[:, 2*x_dim:3*x_dim]\n",
    "        u_t = data[:, 3*x_dim:]\n",
    "        u_t.requires_grad_()\n",
    "        \n",
    "#         if (epoch==0 and i==0):\n",
    "#             writer.add_graph(enc_dyn_net, (x_t, x_tplus, x_empty, u_t))\n",
    "            \n",
    "        x_full, z_t, z_tplus, x_hat_full, z_hat_tplus = enc_dyn_net(x_t, x_tplus, x_empty, u_t)\n",
    "        l2_weight = 1.0 if epoch < 10 else 0.0 # Can use a more sophisticated L2_weight formulation\n",
    "        total_loss, predict_loss_G, predict_loss_L2, recon_loss = enc_dyn_net.compute_loss(u_t, x_full, z_t, z_tplus, x_hat_full, z_hat_tplus, l2_weight)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += np.array([total_loss.item(), predict_loss_G.item(), predict_loss_L2.item(), recon_loss.item()])\n",
    "        if i % 100 == 99:\n",
    "            avg_loss = running_loss / 100\n",
    "            print ('[%d, %5d] loss: %.3f' % (epoch+1, i+1, avg_loss[0]))\n",
    "            index = epoch * len(dyn_training_loader) + i\n",
    "            writer.add_scalar('training_loss', avg_loss[0], index)\n",
    "            writer.add_scalar('predict_loss_G', avg_loss[1], index)\n",
    "            writer.add_scalar('predict_loss_L2', avg_loss[2], index)\n",
    "            writer.add_scalar('recon_loss', avg_loss[3], index)\n",
    "            running_loss[:] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_dyn_net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.2 ms ± 4.66 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "test_net = AutoEncoder_Dynamics()\n",
    "test_net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAD7CAYAAAAy7bIvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO2daZAk6Vnf/08edfXd09MzPbO7M+ylY4UEGysIGRPBIRzCNigcgSMA4QBkaWwc2ApjJOxARIiwAHEERkhBECMLYYQ/APqALRMOyUAIMA4JLSuBrtXex8xOd89M31VdR2a+/tA19fyzprKr+q6sfn4RG/t21puZb1a9+U7+87nEOQfDMIxhxzvpARiGYQyCLVaGYeQCW6wMw8gFtlgZhpELbLEyDCMX2GJlGEYusMXKMIxcYIuVYeQAEfmMiLzjpMdxkthiBZsIxmgjIr8rIu8fsO8LIvLmox7TfrDFyjCMXDByi5WIPCAiKyLyaPvvCyJyU0S+I6P/LwD4dgAfFpEtEfnwMQ7XOIXsdY4Sl0Tkr0VkU0Q+LSJzdMw/EpFFEVkXkb8UkUfa268AeBuA97Tn9yd3GdfHAdwH4JPtvu856LUeKs65kfsPwDsBfBVABcCnAPxan/6fAfCOkx63/Xd6/tvnHH0WwMMAyu2/P0Cfvx3ABIAigN8A8EX67HcBvH/Acb0A4M0n/f30+i84wnXwxHDOfUREvg/A5wA4AN9/wkMyjBT7nKMfc849BQAi8oe8j3Pud+60ReR9AFZFZMo5t36oAz9BRk4GEh8B8DoAH3LONU56MIbRg73O0UVq1wCMA4CI+CLyARF5VkQ2sPN0BABzGCFGcrESkXHsPAp/FMD7RGS2zy6WJ8c4VvYxR3fjhwG8FcCbAUwBuHznNO3/72V+D+29MJKLFYAPAnjcOfcOAH8C4Lf79F8CcP+Rj8owlL3O0d2YANAAcBs778B+sevzvczvob0XRm6xEpG3AngLgJ9ob/opAI+KyNt22e2DAH5ARFZF5DePeozG6Wafc3Q3fg/AiwCuY+el/We7Pv8ogNeKyJqI/HGfY/0SgPe2+/70PsdzJEjbAmAYhjHUjNyTlWEYo8lIui70QkS2Mj76XufcXx3rYAyjB8cxR0XkPuxIxV681jn30mGc5ygwGWgYRi4wGWgYRi7YkwwsSNGVMHZUYzH2wCZWbznnzp70OEaBgl9x5XDy+E7IYkaQ9UfGDicJj+9oxrTRWMqc130Xq3Yg5BUAKKGCb5XvPuThGfvhT90nXjzpMeSZ1LwOJvGmSz+68wG/FhG9OSVOOm3nDyZIUvt4GccK/J7nS8Fjyhjfrvvs1m8v8HF2e300yLkz9v/U07+aOa/7fuvOuavOucecc4+FKPbrbhi5gOd1wS+f9HCMAdi/NdDz+/c5IM9+/PWdttN/jPDgj/79kZ976IlPegCjDz8BodnS7fRk5Sql1D4upPuiRuF+od5qTvTHc2O0f6Lnk+1m7zG1IvpDn1qS6fF0v63t3vvTdSCiSUTnTlbXtP+9F7QPXbds67Ul45XUOVLfAT0O+csUU03fx65PaYS9YDcMIxfYYmUYRi44FKdQ8Y9GEj74Y1/p/cERnW/YcbFpv+PElQqdtjRUlqWkH8syAG5M3+u6gt5e3npNj8XykuSe1FVauRK9H6aX8yxHU/IromMCkC09n5tQC76r1tCTmSntc+GMbm/pnEtKoQ6pqG1pkLQE4Ep63QlJQp9kZDKh7wm9jYwxdWFPVoZh5AJbrAzDyAWHHhsoA/qgGIPh4qR/J+PQcBWSX/zVT09om6RfMpO2wiUFkj1Vko5llZRs2c6Ul57eR/GY7uuTBY/lFuIuixrLyIJKNpyj5KF0HWxl9JfUGpicUWfZ1LVtkWQNu17LkJUyuK3hjm6s3LMPWyJ3w1YWwzBygS1WhmHkgqNNEXNKrXYHxqx+J0Y0o1IlvLHRaaecMbPCXwC4gMJqWr2dLtlhM5lVeekCcrqkfaVFMonO523W9TjjXdEl9PpAquQgSvIymleJJw09n7+i1+0tr3bagZvueXwU0stI9V61UvpzKm39pu4TrpOMrOl17IY9WRmGkQtssTIMIxccW6bQo3IcHRXM4XM4EJIqyZTKmSTo/e+6X2t2/a0y0PkkCSOVbylnU5J7bFUTdvLMMJbFUypZo/FC+kPax6MYRd6nNa5WQr9I547VidTbVAnJY2JZHM2m00b529ovqNO8JsXcmNNxlDfVIRXLyMSerAzDyAW2WBmGkQtssTIMIxecXHWb8NQU1smmKwjWOEHaLgH+tgblerfJdYFcDFJuBfX0O6v4DPUjr3L2YE8F7sa6v08uBuzt7eh9Enx1BeCAaOlyofA4uJjzUNE+fkPbcVnPEbI3PHuX077JuI4jvKHuDQDgNfU7SNitgZzWebx3uV1kYE9WhmHkAlusDMPIBcOhxY4hRfJQkJh7wrDDQcMunNEP2GzvVL67VUrVC8DnwOQy5baqaMCzo1xQ7EoQdMmpTn/OA0WyKuXxnnQFMrOHObnF+OQtLpG6HPhbdKx1Cj7e2NRz11S++ufP6faV9Ljl5u1OO6DXPcm2nts/P6/n6PoOs7AnK8MwcoEtVoZh5ILhkIHEqOXDsnxUeUA6Ab4bl9XKVVlSi1pQo3xP69VO23GuKKQrwyQ31HPcf9X9ejayICbzKg9rrznfaZdfVkske6fHJZKE4S71ABPKv0WWt/CmSrzWrErQuEAVe86r535pUaWf19TvoDVJ1sCu6Au3TVZNStXslSlfF1syZylAehdFOForg2EYI4stVoZh5IKhk4Ep8hr8bEHJOcN1pBJLq2icgnvZ2HZeA28lIrkFAFArWUDpgXn31oLKniTjtUc0rRKtOa23aWlZZRWPKS523St02IRK1K+/QdMab13UTh75trIz6/r9eq2lVZVuHvkzVwoLqVMHt1UGeqsqZ12N5CEFc6dSSe+CPVkZhpELbLEyDCMXDLcMZLwhX1cHrNBhDDeFLU4hrM2E0hXXLqtEK91Ox3dy/qZ4VtMGp+L7qLKLRymL2dIHcvJsVaj/jMqn5rj295tpp9DqeT0f+X5i6yG1cIYTVHiVDItxpPuOjakj55ajXF3UbhXT8ZGpzz59udOeflbP3ZzQc4y/RGmXd6HvYiUiVwBcAYASKn16G0Y+SM3rYLJPb2MY6Pu44py76px7zDn3WIjBXoQZxrDD87rgl/vvYJw4+ZGBzAEcR7/283P9Ow2IS7hQY0Y7pnZX+Jak9tHmq979d4czQGNw7qSIaeiPVNigNCtkISPXRgTV7DQ/8SSlGqb9/bruU5+nVDCcBpmcJidepLTElMplfI3G13VLOE/PfesN9KHPlXn0fFMkCWsN3XdhUq15saPKOAlZEiU9sRfX1UK6/RDFJdZ1uYmLeu7K4mBW/yF/EWQYhrGDLVaGYeSCfMpAwgV7cxwVL/3I6gd7s+Il9PjryGLDkjAlD70MeQjAke+oYJc4L+PoaZvDwi2VaFzppjVFToz0T3xcSs+/pMhZOenwnGKG5g07YAY13cGrq8Srn1OpyNI0GtPbtz6THkf9jI6jOddbqhZLeizf0/EVAu1f8NnBWdvNWM93s5qubtNoaLyk0GuQxoy2A0qWmhQGe2ayJyvDMHKBLVaGYeSC3MvAFAM4jorX/Tc9hge9Y/pY+kmG9NuzPNwZsDZjk4HDgEdOnVzUINjOsPrF6dcKbB1szKmrj59wzKHKJJZALANbZ6jAKsUrbt5D2UfpFUMrrcRQvUjWx0l12pyb1hQx0yV1xrxY0dws3zTxcqc94WufkHRtNdFx/OXqw6lzP9G4p9OmrDJIaLWJKrx9sLlvT1aGYeQCW6wMw8gFoyUDmQzHUc9Luv52vdvk6JbQ9iRD+g0iD5MuJdHlI2qcJG3JF6ypmcpRiiKvTg6RlLkzXNaCCgDS8479gUmysQWxvKgyK67o7SgUMxiRAyU7U5JPJ2oXui6HPvN9qhVI8/+RqRud9hvHn++0K546oY5Ru+5UvracXsObpp9NnftGTcOXnp8nK+qmutPG+8j+ZE9WhmHkAlusDMPIBaMrAxmyEnY7hXbLwl6wk1xEz6+DyEOwNQQ5zXx6mljWmndso5JQJVA4ozJHGl3l4yktjF9Tp8vWtEogrvfXOKPb/YZa29jhk62BzSmaZ3T3NhYoThCAFPRY3/UNT3fa3z/7hU77cqD1/lrk6VolubeZ9A7ynvZVLj9UWEx9dv2M1lu8dluzojZn9F4rLdE9MuD7EHuyMgwjF9hiZRhGLjgdMpDwvIPZ4Iqh6rqYHf1IHrLRL7Z/D3IFy72kSgFsqSIgKvWSqbQ3JmcE9dbJoXKFii2MkWMnOYWyBbC0RTGAZfWgdOTVnFDJwup2ep5NzauV8tsmVQbeG2gRi3NkJaxToOr1phaJ4LQwvmh/dgo92/V244HScqddKalMXqPCEElIzrDbgxVYsTvJMIxcYIuVYRi5wBYrwzBywal7Z5V0BRMfpGhOISPwOYrt34Dccae8S1ldCTxqu4K+IIqn1JzfmE3XJfAifScal/T2CjbVE5xdFyRV3YY82BOqQrNFwdVcfIfeFfld76wScmH/Uu3eTvtiuEq9qp3WSqzvxW5H4502uy6c8TUI+nJ4U4+SpL+Dp+rndex0j0hR25VFvVb27t8Nu6sMw8gFtlgZhpELTocMpAKk3Tml2Nvc8wYzod4hTg641lNKZUcuFU9+6BvpJBnVcf71Hxzs3EZvWup4kkxrlZZ4kszuBXJPiNKuMOGGmurjst5eEe3PQe/BLSo0OkHe7CsquUrbuj0iT/itC3rMwnp6Xo+XVHbOF7RCTcvpmEIqqRRK1LOPD3ZvUCn8Qutsp/3qggZEA8BCQd0j7p/WiIASRYIsX1zQc1fJB2MX7MnKMIxcYIuVYRi5YHRlYNw7QDnpkm78SM6WQs/v7eme5bXO1pfU8TlHVpA+5p6r4zTt35Yjo53Pym2r17lMqIUsWCYptaDBuRJlR0REVJCUg5GLt1Uqts6oB7wjr26vScVPyXqIDGtg3FUsfbup0upcoCmL7w9XOu0aRULfS+VmxuTFTrtOkvDvG2pVZLgPALy6qLKwNKNWzS8XNN3xnz+i112takTAbtjsNwwjF9hiZRhGLhgtGZj0z03lku6/M1IQk3zLknsp2ZiRHjmrYs5u52Z5mDQtB9ax0HYKTQUyF8hJk14reK2453YA8LZV9pSWqB/t4+i4cVnP51FR1YQdRGnfbUoT7FEKK7+evpytqloNa+S0+dntS532AwUNOF5rqey8EGgQdIXC8tkRNKHnnNcV1PIIAFPkab1cfKrT3kx0TAvT5zrtl2fU6robfRcrEbkC4AoAlFDp09sw8kFqXgeDvTMxTpa+MtA5d9U595hz7rEQxX7dDSMX8Lwu+L2zYRrDRe5loER7c+TczSmULYNR1F9+scTjajiBr9ubkX7Fd+XSor9T5zbpd8xIRwYm52Y7W+vn1WJVXNZ50pyi6jab6XTCTPOMLoJRpXca38I6ZT+jLl6DJCHJRi4Iuj2nO7Qm03OrUNTjrlCs34WyxgY+2dCSOOsUG9gsaZHTi4FaQWtOH1YmPLWatrrerSzHGnP4paY+tdbIZPnSsn7PYdViAw3DGCFssTIMIxfkUwZmOHwOQrcMZMtbTNJvEHHJMpCthCnrYZL9iMuVdZLEpN/J4TpOoY1z5KRJDrleVc1t4ZZa8Fi+A4DUVRYWl9XRMrmox2UHUZZ1nCKm2MywPvLrAsqoHI2n74n5isq0CTIV3oxUlt1oqXPrHFkAz/gq455sqtWOpd+YqGNr993YdDrGx2v3Uz+91iDkSj6DpRq3JyvDMHKBLVaGYeSC/MjAARw+BztOlzUw2v96XY3Uya08ro5x/LjbHYvIsMWxW54ax4laAwOqKiMRpfApqfTz11VWRTNptwcZ51Qy5PxJVWya49qOSmTRq5DsbOn5mpOUsdQnKaVqDV5zsPmz1NLKNbVYrZp/u3250z4faizhA+QIes7X7+ZTJO+eTNd5xbWmWvqeWNN4wgJZyQtUJWqraDLQMIwRwhYrwzBywXDLwHhvDp8D0Trg+lzsPaZGnWK2yMrHKWJ2c0h95js/1mnf/3/efrAxGntErYHBKmXurJO+IQtXNK9Sii11ACAN/dsnGZmEKhdLaxQnSEbg4hrNFbJEhlWaT7H22Z7VORdspef1Wk3P9/WaFnCokvR7dOIlPQdlyeWsoVXKDvpcpO2ntvWYMyHpUQDLTY31K/l6rItlzSC6WFGr5EZlsNhAe7IyDCMX2GJlGEYuGDoZ6A7g8DkIr/m55w+0/9c+oCk20B3r10Yy0sXc1Z/Cwlj6ObZQmpXweGEnzwwLNIWBppyKAUCaFCso+ts1x7jwh7YLGxRfSllHi4taMCKpqHTjYhWlNaq9F6SfO7aeU2n1zPRcp90i5+PbDfUq3WioxXF1VuME5wvqLHqrpTGG9xU14+gKe6cC2CQr+Vak1tEN2v7y8kyn7dUGe2ayJyvDMHKBLVaGYeSC4ZCByRFY/YYEtgx2p4jhuMGU1ZA7RdTHJOHR0bb2JRXK2TapFjX/Fa1/x5k70SW/4FPG11AlV1DXX7WySAUjJvQWLFxX6QdOfUSWyOCW9pF7VFaN3UjfQ9vndBxPvzKv5x5T5+VGvXe9vnWqU/gNM3rdAc3lr62pNfA104up/a9VNebwdlUl5fUNtQAmDf1uwoaliDEMY4SwxcowjFxgi5VhGLng5N5ZtaL+fYaRjGKkWQVLOVj5rrTGeyWyd1ZHRtvNwKtRQDoFJbtpdQXgyjNRJf3ep9DUeR2P6f5+s/dvH1a1v9SpSsyyvivyq3puV1UP++kv69zaeLW+JwKAca1Timqk796SurZbl+geJH+M0rS+F6vHen1zoW4PSvr+qhqlazO0KJ9bRFEaRcr/Fo5RPqzCYMuQPVkZhpELbLEyDCMXHJsMdEcRlHwSpKQYFUXN6C70z8Fu30AqyNlcFE4Maag8kTJ5jpP0a42rNIrL6XTUfl0lUUT9grrOkGBD82FxwVP4lFZ7Q6vKeOS6IOQaISuad6q8lM6r5Twdh0fB+6U1cqWJuYir7utdIklIOaw+99LlTrtS0u8p7Crku7ah7gpRQ8+x2dRxBKu6vbRirguGYYwQtlgZhpELjlYGjor0I4RkYNq+Q5KQZdygFkDeJ+7ttf7Qv/98p03GHuMQcSFJI7bszai0SUL9rf1G+gWAI5nG1W1a0+oVzhVwGLYGSpFk3JymCcY2ScgJDSDmPFcAULmhxxqnvFpejWSuU4/y6rxK0JtPaEWbVygIunydpDAZEmtdBVZZ1PkZRZvGX9ZeY0uDrRP2ZGUYRi6wxcowjFxw6DLwqPNRnTQP/9QXOu2nfv2b9QN2EGXpN6hhj3dJWQb3OEBjf7Qtbq5EjqBFCra9oSl5fQrO9dYo+BiA26S/6TVIYV7llGyT3Ktq4VAOgvbnznTayZTKvXhBnT+9lk6OaCx9K7M8ZeulT4HXHjmqshTzyJG5fkaPy9V0mprZGZQpGQAQbur8nXxZz+G3KF/Xup6vtETfwS70XaxE5AqAKwBQQqVPb8PIB6l5HUz26W0MA31loHPuqnPuMefcYyGK/bobRi7geV3wy/13ME6cQ5GBI+PwuUceetfjnfbT/+WNnbbs500g57Yi6ffgf/h8j87GYSN3YjsplXHiq3yK5vXpy18hPdToqvBJZjIXUdwfdXEVtQwiIHOZ62055pTFbG3kjMpBLR1ryxIxWFXro0eyUxKVlMGabi+sa/ri4rp+B6XbepztedV+26vpdx1+g+TlIscfarPyojq9pqTwLtgLdsMwcoEtVoZh5AJxGY+ePTuL3ARQBXDryEY0vMxhuK77knPu7EkPYhRoz+sXMXy/8XExTNedOa/3tFgBgIg87px77FCGlSNO63WfJk7rb5yX6zYZaBhGLrDFyjCMXLCfxerqoY/iBBGRHxOR/ztA15G6bqMnx/Ybi8hnROQdx3W+PnSuW0TeJyK/f5KDyWLPi5Vzbqhv2qOaBMN+3cbByctvLCK/KyLvH7DvCyLy5t365OW6TQYahpELhnqxEpEHRGRFRB5t/31BRG6KyHdk9P8FAN8O4MMisiUiH25vdyLy70TkORG5JSK/KnK3n7mIXG73DWhb50lNRB4Ukb8QkfX2cf7gCC7byBl7nafEJRH5axHZFJFPi0gn2llE/khEFttz7S9F5JH29isA3gbgPe05/sldxvVxAPcB+GS773tojl8RkVdE5IaI/HTG/t8hIte6tnWe1ETkW0TkcRHZEJElEfn1/t/W/hnqxco59yyAnwHw+yJSAfAxAP/NOfeZjP4/C+CvAPykc27cOfeT9PE/A/AYgEcBvBXA2/cxpP8M4NMAZgDcA+BD+ziGMWLsdZ4SPwzgxwHMAygA4EXjfwN4qP3ZEwD+e/tcV9vtX2nP8e/bZVz/AsBLAL6v3fdX6OPvbB//HwH4mX5SMYMPAvigc24SwAMA/nAfxxiYoV6sAMA59xEAzwD4HIAFAD+7z0P9snNuxTn3EoDfAPBD+zhGC8AlABecc3Xn3CAv5o1TwD7n6cecc08557axc6N/Ex3vd5xzm865BoD3AXiDiExlHGc//Lxzruqc+xJ2Ftf93g8Pisicc27LOffZQxzfXQz9YtXmIwBeB+BD7R9vP7xM7RcBXNjHMd6DnXDMvxGRr4jIfp7OjNFlr/N0kdo1AOMAICK+iHxARJ4VkQ0AL7T7zOHwOIz74V8CeBjAkyLyeRH5p4cysgyGfrESkXHsPAl9FMD7RGS2zy5ZLvn3Uvs+AK/06HMnnJ4Td53vHNi5RefcO51zFwD8KwC/JSIP9hmPcQrYxzzdjR/GzquKNwOYAnD5zmna/99L2MlB74fOvSAiPoBOKIxz7mnn3A9hR6r+MoBPiMjYXUc5JIZ+scKOLn7cOfcOAH8C4Lf79F8CcH+P7e8WkRkRuRfAuwDc9XLcOXcTwHUAP9L+1+3t2NHiAAAR+ecick/7z1XsTATL5WkAe5+nuzEBoAHgNnYWi1/s+jxrjvciq+/PiUil/eL+x9HjfgDwFICSiPwTEQkBvBfQpHYi8iMictY5lwC4k0r1yO6HoV6sROStAN4C4Cfam34KwKMi8rZddvsggB8QkVUR+U3a/j8A/C2AL2JnMn00Y/93Ang3dibKIwD+H332RgCfE5EtAP8TwLucc8/t7aqMUWOf83Q3fg870uw6gK8C6H4X9FEArxWRNRH54z7H+iUA72335Rf4f4Gdd2x/BuDXnHOf7t7RObcO4N8A+K/tsVQBsHXwLQC+0r4fPgjgB9vv346EPQcy5xERcQAecs49c9JjMYyTREQuA3geQOici3bvPVwM9ZOVYRjGHY62yOkR0X7s7MX3Ouf+6lgHYxgZHMc8FZH7sCMVe/HatqvOSHAqZKBhGPnHZKBhGLlgTzKwIEVXwpG5URh7YBOrtyyt8eFQCCquHB6mc3gfWM3IoFVwTwcb9cXMeb2nxaqEMXyrfPfhjMo4EH/qPvHiSY9hVCiHU3jTA+1ghKyFhCuNe9K7TzdZ+0RUuo5LcXkkdJIDuitxnS4q3zXQQpn1aoj77+f10QDf56e++kuZ89oqMhunktS8Dq0icx7ou1i1o7yvAsCkzOpy6vlZuxwaT330m/QPKgL68Du/cOTnHnpOZ13ZQ4Pn9VR5oTOvpVbXPtSWUpF31nZXgd/kjBYORcBPStTJ13tn81UqPwubeqziohoSpaXbt+/T/o1pvX3DWvpJrPK8FhH16DqwXUcvohsUpkj3tv/g5U7blbTgqbelx2ncl44sEvp+EnqqK97QMfGx5NpSzzF1Yy/YDcPIBbZYGYaRCw7FKVT8o5GEr7rypd4fHNH5hh0Xm/Y7Mu5Il0azs0kKKlXQamk71O1uIm0dd6HOTaGX5N5WFb0oreh74GCjd1aZpFLotOOSPl/ERX014rz0c8dYSyNpkkk9h5RJztKL/kCTi8CN07tpuoZ4jPYl1em89It6adJ1c0caYzym1xScJ+PfLqVW7cnKMIxcYIuVYRi54NBjA8W39e8wcbGlyzpOHMnAlG/U/JlOM2E5tAssh5IplYvepmZR8bcp8QHJJEcKVGK1rpWX1AqXBOVOO6ym50lS0TFGU9qOSypTCyt6rGCDrI/bJEdJCrOVT+rax2vpOAAgKeg52AKY5deVlAZbhmxlMQwjF9hiZRhGLjjaFDGn1Gp3YMzqd3KcVQfHlIMoha/ELFu6pQ1HtpBTqH9rU/d/WdOd8w3oOPSG23QOL1K5V17S47cmSDcC8BpqvQxXSSLOqmRjuZayALJlkcaR0PXIzDidO72MVM/r3+XJGW0v6vfZnFJrYPH2YK867MnKMIxcYIuVYRi54NgyhR6V4+ioYA6fJ8wdqUWSy42pZIrOqDUvKZJ86lKBwbb+jo4s446scxLqbZdMkCwr6navqcfxVlVCshNqbUGP2e2YWZjS48ZlOm5LJZdkJE5wND7Jyv5AlsGonH7mCepkNSRjp7+uVtCgvPelx56sDMPIBbZYGYaRC2yxMgwjF5xcdRt7h2UuCsPEnXcwa+pxLRSw7E3qOyCfgpJdIX0LJRTIHGyQ6wO7H5xVb/jmZEn7+NSH3lk5zqVFjxfstc4BzgDg1yggm/NLkfuBV1P3BnbTYNcFIY/+kIKj2aVh7Hr63PU5vSbJyCgabFFg+IBZUe3JyjCMXGCLlWEYuWA4ipyeluBnC0oeelxNzesIVKp4W5QTarOm/SfSdQl8klMcvJw6R1G9t9n7O6yqzBLyQOf7Q+q6vfIipQnu9qR/Tmub+hc1V5VH5/ZurXba8a3bun2TrmlCPdVZErILhb+ZzsNV2SIJStJRKFjakWuFv6rf526cklXCMIy8Y4uVYRi5YDhkIDNqVkKz+OWDtoyShXndRLKHU/p6JLk45TCQ/tff2yB5w7UCSdaVlrRPQpbFlNSsk6VuRqvbSJUkazmdY8s9fLnTblDwcrhKMnVeg4w9qsoTkYWSrYqcipi94iVJW/yCNcp1tUmWU7L6scd9a4EqAj2NTP1qczUAAA9ESURBVOzJyjCMXGCLlWEYuWD4ZCAhXj7XUnfQ0t/G8dO21CYTKoF49qUChZtkqeuSgSwL/TUKQKZ8UUKS0KurtcwjyZUqpDqrMon35fxXSalrHJRzqzWp/VpkxYuLeoVhNe653Yl+H+Dq8fR9jL1MchRAPEkWxyZdN8lFF1IuroqlNTYMY4SwxcowjFww1DIwxbBbCc3ql2/asqZ+TmVLmarKcBxd7eG5TpsdOQHAr2q/1v3qjBmsqVSKzqijZSpVMJ0vZAlJFWY4LxZTuy/tgLq1wNJPJVt9jqSYz1Vz9BwxKb/Curabk9r2yQ906VvS1W0k0vPN/61KwrFXtN+tR7Q9cT39HWZhT1aGYeSCvk9WInIFwBUAKKHSp7dh5IPUvA4n+/Q2hoG+i5Vz7iqAqwAwKbMZiVCPmQPEEn71vef7dxoUNvpxflvaLknv7d2fCanIh372i4czPiMTntdT5YXOvE7JOrLIubLKJE5djLjrluB92PFxpndVmcas3oIBFyq9fK7T9Jo6pu0FlXuO3ozUZ9KvSbbP6bkbZ2i8XH3H07E2Zig9TUvb2+d63/IRP7d05XZOirrP5r06Lr+pkjAh46UMaDw3GWgYRi6wxcowjFyQH2tgFnt1HPXSj7US7M2B06VkXYbESznPoXefu/7uSvFhHC/teD+/ppKLY+Eas2qF85v6wwXN9I8qLa5uo78pHzdVkJSmIztNtia1j8Tk4DmuE2p7TtvVe9LzOjqv5rqxKbVEJjRnA1/HvhGohdIr61iTOi0RfO80yYoZdc1dOkdMxkuuglNaYbmMgbAnK8MwcoEtVoZh5IL8y0CCY6Uy6ZaB9LefIQmThGKlOL6JHnf3Kg+BlAKAmAw8WdpWPG9DJZObUZNXuKXSKODMmF2vIRw5L0dlit0LKK1MgeYN7c7yMqqQxXBab9ONSyS/aAK1zqQdKy9d0MyfD03d7LTPFtTZtEYmuc8WLnfaE0W9vtBTWXttXdPThL5u326k4xK3b6vlszml11qbZwdY7V/YHGzu25OVYRi5wBYrwzBywUjJwBQZVkK5Swb2/syjtmRIvz3Lw6hrTBZOOHRwdlB/g35fXyVhzGlgqDgCkLYGVl7aQC84Eyer/6So0i9c1xhDdvhk+cQyMJzoKtoQ6rjGAv1sgYL9PPLGHL+gfR4sLXXa15qznfYT/r2d9nJtotOeLlMmUwDP3dLvygU6yLjQW+5FJZOBhmGMELZYGYaRC0ZXBjJcd61LBnpef6fQINBnb5Z+g8jDhEPNBhutcYLE1xc7bf/cWW1zmfiI5kz3P/f8d0PnTTSnTpegeRNTRk8v5nLwvS3b4ZbuW1VVhu6ygeOhyrp/PPV3nfZrC1orcCXRcz8daKGM27GO9eHSjU77yUDjar/t3HOddsmjzKkANhvqCXrzhloQGy11dPWaOuDmpMlAwzBGCFusDMPIBadDBhJywOU5LQl7Sz+Q9Esw5BlOjR3a0swrq6UuWVHJJFyXjzRXa6ErFxYpxGBdHUw9KvseTeixkgLNG64XQXGFxXVyFiV5GGxRKpdG+lauRWqxLJAJ8atNrRW4lqjV7mvbF3VfchadDbTu31oznRH0Dg+NLaf+vn9aHVJvLqkMjMsqYaOKtqd3qRXI2JOVYRi5wBYrwzBygS1WhmHkglP3zsp1eyocYLnmfEAcRhrbvwH5404KYnpnJZNqwm9d0HcvrTG9be6qbkNFS13Y+30lv4/ijMBcjJQDnzntr0enC/V1EmQ87cG+UFbv+cVIx76R6Hun9VjbG5Fe9/PVM512wVcP9tW69vfIff7zq5dS537yFU3JLNt6TYUVvS+mntWL4u9jN+yuMgwjF9hiZRhGLjgdMpA8g1OBxUi7HPje3iKLE3fAHFRUZDIp6Rif/PXXd9qZ1XH+7R8c7NxGT0Q41xSn7u1dtSYqp2+hYE2DejmoGZG2vXF1DQjXVb6xN7tkebm3KM/VjG6v1ylVMtJByuxh/kxDJdp6pLJuMtBxny9rzqsC6c6tVrHn9ofH064LLBEXpzTgeWVGK/Osg3OFYSDsycowjFxgi5VhGLlgdGVg0jtAuVsGZuWh6k5/rIfNCFhOektCDpx23WmT91gdR5oHlJ1GNu154MgC6EKSXw2VPX5Tt3e/CYjHqQpOVV3Sty+rRY6LpMZUPLW2oPvGpOrYGlg/oxOCDHgoldLBxK8Z0wDkmCZRSN7s8wW1GJ4LNM/VzaZKt5mg1mnPFtX8OEk5slj2AWmJOD+mGq/Roio9r6fCrc/o+XbDnqwMw8gFtlgZhpELRkoGSjSANe8uGdhb1rE1htf0JCNlMcu9QSrmdO+fWR2nZdLvWGhb+FxRLXURWe38qsqscEPlXcriB8DbSjtn3qHytFrb4jMkNcmyWLpNwc5cGYfSAXPRUJ9OVa2lK8x84uVHO+3vWXiy02bnz3GScstNDcieDlX6zYVqGdwo6L4z1KebCcqlNV8iGUjFWldqaomsVwYrNGxPVoZh5IK+T1YicgXAFQAokW+EYeSZ1LwOJ/v0NoaBvouVc+4qgKsAMCmzw5eZN8Pql90/WwayZTCO9DE8S1z6lNvKS1XGoZhBOk53Lq2UpZDOnTQtB9ZRw/N6qrzg7uSoYlnnUfriaFr1FxcgLS1pzioAcEVy1FwgucdFTqldXFPp51Ea5AIVPG3MqmkwIStha4zmzFbaKXR7SsfxpY0LnfajUy932tcamtsqICshW/cuF7RA6o3mtJ7bUSUeSd8hY77K5JWmPuAUfbUAcopjVzQZaBjGCGGLlWEYuSCf1sB4j9KP6d6VrYHdRUj74GekOOZ/A7qdUJlnvvNjnfYDf/bjezq3cTS05qiYaai/Y3FRHSLdOY1x87bTzpgJVcEJN/UzR8fiOcjOpi7waLvuW3Qqy4or9IqAPFKb59NvaBx9dm1T5dut7XH04nsXvtJpf0NRY/18qsk04atF8x+Ofb3TfraplXEAYDOm1NDk4cxtflUipcFicu3JyjCMXGCLlWEYuSA/MjDeW/qWTLqDuVKJHvfmgNmM1EIUjqsFJM6qetMFS7/d5KJxfHjbOiHC6+rQGM9p/FphSSWhbKedQN0YOZLWVMq1plUaOXLyjCo6hxrTejtWlnU+tcZ1e2lV5VMSqCT019K38mqssYjeNjk1l0l+VfRaf2f1TZ322Sm9bnYo/ebKC512NdFxX2tqZlEA+Mr6go491jGeq2gs4uvOajHZpwMtJvsisrEnK8MwcoEtVoZh5IKhloFurw6fAyBRt9zam/xyYe8xteqU5TEjTvDu9DQZ8YAmCU+M4KZKlWRM49fYWTSeVAnkitkOvCz9hBxMA7ISRnQsjg1MqIhCEmo73NLjxPfqubuT3CYNfQ4pL2p7+7z2CWZUBk6Nq3PrfEXjAbnI6X2BFn0NKW/NZzhgEcDSlkrmMxXdv+zr9Z2vaCHUlzbVOXU37MnKMIxcYIuVYRi5YPhk4GFZ/TJ49fueOdD+X3v/A/RX71DJtAzsvR0AErJEOnZIjTMyiBpHTjKlDp9xheLt6KeLKnrbBNIl7bneX0zzgGI/OY1MSPJSyEFUmiqZ/Hu0dh/XJZx+Ts91czx9K7PRmxN58muQ5rrKt5s1vdaCr2P6Wlkte4+Wn++0v62o87VCsYAA8MZzL3Xa5ygb6YOlpU6brYnFIF17MQt7sjIMIxfYYmUYRi4YDhl4kFi/IScVA9UlA12i1hzHn5EM5Md2MSvh0XEn/o4s0CzdUhlBx7Jvm8QnB8yS/l4+F/ugPlxi3tukdDMUD8gpaZioRLGLa10fUgbScIOORWMqrqr0a01on+WSWvOulTWu8NkJjQG8HqlMrXhpGchpYZhGot/bI5Xrnfb9E2oZ/EzPPXewJyvDMHKBLVaGYeQCW6wMw8gFJ/fO6ohdFI6MjGKkyKhUE5NLQnda470iVunm6KHf19/Q/E1JRQOUvSg7u7fXJE/3or4T8ilAGsv6jsab1XdCrqTnkDX1Ii8taSUZb0PbEmmwcu2sulwAQOUVHWNrnNMoa59GluO49L6+81QI9R+UdHz/y6Un9sWSXtM3jl3rtM8G6sZw3tf2J6M3ZAwkjT1ZGYaRC2yxMgwjFxybDHR5lX1dsCtBlhhwnNbYG7AgUGJe6ydKp7oNeZFT299U+RWNqwm/Oz1auESB0AXVWQm5KPj0PsCVSV6uqLSKlrSqjN8k14BQpWX4gsrUyXP3pcbRmCSXCJKtXIiGXWHYpaF+ScdXben4fuv6d+nOF/+80/x6Xb3cAeBGXeXpfEGvaaml2z++cW+n/cRL2t4Ne7IyDCMX2GJlGEYuOFoZOCLSj/HIIsdqLSX2sqJId8P19lTn9gM/8zedtoaUGodC22Oci5RyMDF7nbPFD12BzMkk5cBKenuhh/MamBxNUGrsbZV7wUWVVmwlTMYoR1aTKuN0jWP8Gh2L0is3ZvV8dUo5HFIANp5QD/SX5vR6XKDX867lH+y0W830MhLX9bhfn9eUxRGdr/akWgwL64NZue3JyjCMXGCLlWEYueDQZaAb4aBkAHjwPz7RaT/zgUc7bXYETVkAvQEdOelrS8nA0VPSQ43UNNeUI+mHTU3PG76i6X1RT1e3SdbUcbJw74VOO5qf1E4NlWgeVUhiqcnzxpH0i8ZVEoZLag30WunXDR7dh3GZpC3Ns7FXVB42p2gpICfP0goNe0a3b05wyub0Mw+/+dj8sla+8Rt6TRX6CidftiKnhmGMEH2frETkCoArAFBC79QPhpE3UvM6nOzT2xgG+i5WzrmrAK4CwKTM9jRtjYrD51554N2f77Sf/eVv6bT3FQOYsgDq5vv/09/06GwcFJ7XU+UFh7blTuoq0ZJZzevkbalTaLK4rAfyu3JNeRQLWlOZFt7QH9VxcdIVlZecwyqZHtfjkNWv8KI6nbJs5BTKABCs6bmlTlZNfi1BFsTikrbHyYG1OavWwNZtcjRtqXNqrIoQABBu6lhCrZcKoVxhY4s0pgEN5iYDDcPIBbZYGYaRC8S5AZ/BAIjITQBVALeObETDyxyG67ovOefO9u9m9KM9r1/E8P3Gx8UwXXfmvN7TYgUAIvK4c+6xQxlWjjit132aOK2/cV6u22SgYRi5wBYrwzBywX4Wq6uHPop8cFqv+zRxWn/jXFz3nt9ZGYZhnAQmAw3DyAW2WBmGkQtssTIMIxfYYmUYRi6wxcowjFzw/wG/2wQjmT10iAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_img = np.random.randint(len(dynamics_test))\n",
    "    data = torch.as_tensor(dynamics_test[test_img].reshape(1,-1))\n",
    "    data = data.float()\n",
    "    x_t = data[:,:x_dim]\n",
    "    x_tplus = data[:,x_dim:2*x_dim]\n",
    "    x_empty = data[:,2*x_dim:3*x_dim]\n",
    "    u_t = data[:,3*x_dim:]\n",
    "\n",
    "    x_full, z_t, z_tplus, x_hat_full, z_hat_tplus = test_net(x_t, x_tplus, x_empty, u_t)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(2,2,1)\n",
    "    plt.imshow(x_t.reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_t')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax = plt.subplot(2,2,2)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.imshow(x_hat_full[0,:].reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_hat_t')\n",
    "    ax = plt.subplot(2,2,3)\n",
    "    plt.imshow(x_tplus.reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_tplus')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax = plt.subplot(2,2,4)\n",
    "    plt.imshow(x_hat_full[1,:].reshape(img_res, img_res))\n",
    "    ax.title.set_text('x_hat_tplus')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.18369\n",
      "(741,)\n",
      "(238,)\n",
      "(125,)\n",
      "(74,)\n",
      "70.13157\n"
     ]
    }
   ],
   "source": [
    "opt = np.get_printoptions()\n",
    "np.set_printoptions(threshold=np.inf, formatter={'float':lambda x: str(x)})\n",
    "expected = x_t.numpy()\n",
    "actual = x_hat_full[0,:].numpy()\n",
    "diff = np.abs(actual - expected)\n",
    "\n",
    "# Q: Test MSE loss\n",
    "print (np.mean(diff ** 2))\n",
    "\n",
    "# Q: Explore the input\n",
    "# print (expected)\n",
    "# print ((expected > 30).reshape(32,32))\n",
    "\n",
    "# Q: How varied are the differences?\n",
    "print (diff[diff > 2].shape)\n",
    "print (diff[diff > 10].shape)\n",
    "print (diff[diff > 20].shape)\n",
    "print (diff[diff > 30].shape)\n",
    "print (np.max(diff))\n",
    "\n",
    "# Q: Is there a high diff at obstacles / robot locations and not so high at empty space?\n",
    "# A: Doesn't seem to have such a pattern, so nothing conclusive.\n",
    "# print (diff[expected > 30])\n",
    "\n",
    "# Q: Is the diff uniformly proportional to input?\n",
    "# A: Doesn't seem so either...\n",
    "proportion = diff / expected\n",
    "# print (proportion)\n",
    "# print (proportion[proportion >= 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.521121e-05 6.556511e-06]]\n",
      "[[3.0495226e-05 -2.4400651e-06]]\n",
      "[[0.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print (z_t.numpy())\n",
    "print (z_tplus.numpy())\n",
    "print (z_hat_tplus.numpy())\n",
    "np.set_printoptions(**opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to use some z_t and a near by value to decode and observe the full states for near by latent states\n",
    "# However, one problem is decoder network expects an empty image of the full state as input.\n",
    "some_z_t = np.array([[0.5, 0.5]])\n",
    "near_by_z_t = some_z_t + np.random.rand(*some_z_t.shape) * (10**-2)\n",
    "print (near_by_z_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2RRT - A lot of this code (except the tensorflow specific code) is taken from source!\n",
    "plotOn = True\n",
    "num = 1000\n",
    "success = False\n",
    "count_success = 0\n",
    "max_success = 100\n",
    "T = 20\n",
    "connection_radius = 0.15\n",
    "stepsize = 0.1\n",
    "radius_goal = 0.1\n",
    "goal_bias = 0.1\n",
    "cc_cutoff = 0.9 # only accept edges X likely to be collision free\n",
    "\n",
    "# initialize empty tree\n",
    "parents_rrt = np.zeros(num, dtype=int)-1 # index of parent node to each node\n",
    "zs_rrt = np.zeros((num, z_dim)) # positions in latent space of elements of the tree\n",
    "costs_rrt = np.zeros(num) # cost of each sample\n",
    "trajs_rrt = np.zeros((num, T+1, z_dim))\n",
    "T_rrt = np.zeros(num, dtype=int)\n",
    "us_rrt = np.zeros((num, u_dim))\n",
    "\n",
    "if plotOn:\n",
    "    fig1 = plt.figure(figsize=(14,10), dpi=200)\n",
    "plotOn and plt.scatter(samples_rrt[:,0], samples_rrt[:,1], color=\"blue\", s=30, alpha=0.01) # current\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# choose problem\n",
    "idx_problem = randint(0,num_problems-1) \n",
    "x_init_rrt = x_init_problem[idx_problem,:]\n",
    "x_goal_rrt = x_goal_problem[idx_problem,:]\n",
    "xempty_rrt = xempty_problem[idx_problem,:]\n",
    "\n",
    "# In original, x_empty and u are also passed to this call but I don't see a need to do that, validate by running...\n",
    "_, zs_local = enc_dyn_net.encode(x_t=x_init_rrt, x_tplus=x_goal_rrt)\n",
    "\n",
    "zs_rrt[0,:] = zs_local[0,:]\n",
    "z_goal = zs_local[1,:]\n",
    "\n",
    "dummy_input_for_goal = np.zeros((1,u_dim))\n",
    "G_inv_goal = enc_dyn_net.compute_grammian(z_goal, dummy_input_for_goal, self.predict_dynamics(z_goal, dummy_input_for_goal))\n",
    "dense_CC_conv_out = cc_net.image_representation(xempty_rrt)\n",
    "\n",
    "itrs_rrt = 1\n",
    "\n",
    "# explore\n",
    "for i in range(0,num-1):\n",
    "    idx_expand = randint(0,num-1)\n",
    "    sample_expand = samples_rrt[idx_expand,:]\n",
    "    G_inv_expand = G_inv_rrt[idx_expand,:,:]\n",
    "    \n",
    "    if random() < goal_bias:\n",
    "        sample_expand = z_goal\n",
    "        G_inv_expand = G_inv_goal\n",
    "  \n",
    "    # best near neighbor within ball radius\n",
    "    neighbors_heap = []\n",
    "    for nn in range(0,itrs_rrt):\n",
    "        dz = zs_rrt[nn,:] - sample_expand\n",
    "        if dz.dot(G_inv_expand).dot(dz) < connection_radius:\n",
    "            heappush(neighbors_heap,(costs_rrt[nn], nn)) # push 0 in\n",
    "    \n",
    "    if len(neighbors_heap) > 0:\n",
    "        neighbor_entry = heappop(neighbors_heap)\n",
    "        idx_neighbor = neighbor_entry[1]\n",
    "    else: # take the nearest node    \n",
    "        idx_neighbor = -1\n",
    "        neighbor_cost = np.infty\n",
    "        for nn in range(0,itrs_rrt):\n",
    "            dz = zs_rrt[nn,:] - sample_expand\n",
    "            if dz.dot(G_inv_expand).dot(dz) < neighbor_cost:\n",
    "                neighbor_cost = dz.dot(G_inv_expand).dot(dz)\n",
    "                idx_neighbor = nn\n",
    "    \n",
    "    z_expand = zs_rrt[idx_neighbor];\n",
    "    z_expand_idx = idx_neighbor;\n",
    "    \n",
    "    # sample controls and forward propagate\n",
    "    # The may be accelerated by sampling many points in parallel. By then batching the tensorflow call, this shouldn't incur much slowdown\n",
    "    isFree_expand = True\n",
    "    uc_expand = np.random.uniform(-stepsize/5,stepsize/5,size=(z_dim))\n",
    "    T_expand = randint(1,T)\n",
    "    traj_exp = np.zeros((T+1, z_dim))\n",
    "    traj_exp[0,:] = z_expand\n",
    "    \n",
    "    for t in range(0,T_expand):\n",
    "        zp_expand = enc_dyn_net.predict_dynamics(z_expand, uc_expand)\n",
    "        z_expand = zp_expand[0]\n",
    "        traj_exp[t+1,:] = z_expand\n",
    "    \n",
    "    # check collision\n",
    "    y_CC_expand = cc_net.forward(traj_exp[0:T_expand], traj_exp[1:T_expand+1], np.tile(dense_CC_conv_out[0],(T_expand,1)))\n",
    "    \n",
    "    value_CC_expand = 1 / (1 + np.exp(-y_CC_expand))\n",
    "    isNotFree_expand_t = value_CC_expand < cc_cutoff\n",
    "    if np.any(isNotFree_expand_t):\n",
    "        isFree_expand = False\n",
    "        plotOn and plt.scatter(traj_exp[np.where(isNotFree_expand_t)[0][0],0], \n",
    "                               traj_exp[np.where(isNotFree_expand_t)[0][0],1], color=\"black\", s=50, alpha=1)\n",
    "\n",
    "    # can also add the expanded edge up to the state of collision\n",
    "    if not isFree_expand: # the connection wasn't successful\n",
    "        continue;\n",
    "        \n",
    "    # add to tree\n",
    "    costs_rrt[itrs_rrt] = costs_rrt[z_expand_idx] + np.linalg.norm(uc_expand)*T_expand # T_expand + \n",
    "    parents_rrt[itrs_rrt] = z_expand_idx\n",
    "    zs_rrt[itrs_rrt] = zp_expand\n",
    "    trajs_rrt[itrs_rrt] = traj_exp\n",
    "    us_rrt[itrs_rrt] = uc_expand\n",
    "    T_rrt[itrs_rrt] = T_expand\n",
    "    \n",
    "    # or don't break and keep going and take the best over time\n",
    "    if np.linalg.norm(zs_rrt[itrs_rrt,:] - z_goal) < radius_goal:\n",
    "        if count_success == 0:\n",
    "            print 'success'\n",
    "            success = True\n",
    "        count_success += 1\n",
    "        if count_success > max_success:\n",
    "            break;\n",
    "    \n",
    "    itrs_rrt += 1\n",
    "    if np.mod(i,100) == 0:\n",
    "        print('i = ', i,', t = ', time.time()-start_time)\n",
    "\n",
    "# plot nodes and costs\n",
    "max_cost = np.max(costs_rrt)\n",
    "for i in range(1,itrs_rrt):\n",
    "    color = costs_rrt[i]/max_cost\n",
    "    plotOn and plt.scatter(zs_rrt[i,0], zs_rrt[i,1], c=[color, 0, 1-color], s=60, alpha=0.4)\n",
    "    plotOn and plt.plot(trajs_rrt[i,0:T_rrt[i]+1,0], trajs_rrt[i,0:T_rrt[i]+1,1], c='black', alpha=0.3)\n",
    "    \n",
    "# plot solution trajectory\n",
    "if success:\n",
    "    # shorest time path\n",
    "    best_T = np.infty\n",
    "    idx_soln_T = 0\n",
    "    for i in range(0,itrs_rrt):\n",
    "        if np.linalg.norm(zs_rrt[i,:] - z_goal) < radius_goal:\n",
    "            tmp_T = 0\n",
    "            tmp_idx = i\n",
    "            while not parents_rrt[tmp_idx] == -1:\n",
    "                tmp_T += T_rrt[tmp_idx]\n",
    "                tmp_idx = parents_rrt[tmp_idx]\n",
    "            if tmp_T < best_T:\n",
    "                best_T = tmp_T\n",
    "                idx_soln_T = i\n",
    "            \n",
    "    # best cost path\n",
    "    best_T = np.infty\n",
    "    idx_soln_T = 0\n",
    "    for i in range(0,itrs_rrt):\n",
    "        if np.linalg.norm(zs_rrt[i,:] - z_goal) < radius_goal:\n",
    "            if costs_rrt[i] < best_T:\n",
    "                best_T = costs_rrt[i]\n",
    "                idx_soln_T = i\n",
    "        \n",
    "    idx = idx_soln_T\n",
    "    while not parents_rrt[idx] == -1:\n",
    "        plotOn and plt.plot(trajs_rrt[idx,0:T_rrt[idx]+1,0], trajs_rrt[idx,0:T_rrt[idx]+1,1], c='green', alpha=0.8, linewidth=5)\n",
    "        idx = parents_rrt[idx]\n",
    "else: \n",
    "    print \"failure :(\"\n",
    "\n",
    "plotOn and plt.scatter(zs_rrt[0,0], zs_rrt[0,1], color=\"green\", s=500, alpha=0.8) # plot init\n",
    "plotOn and plt.scatter(z_goal[0], z_goal[1], color=\"red\", s=500, alpha=0.8) # plot init\n",
    "plotOn and plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pk-lsbmp",
   "language": "python",
   "name": "pk-lsbmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
