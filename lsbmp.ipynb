{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things that doesn't yet seem to be supported in pytorch:\n",
    "# padding = same\n",
    "# spatial softmax\n",
    "class AutoEncoder_Dynamics(nn.Module):\n",
    "    def __init__(self, img_res, z_dim, u_dim):\n",
    "        super(AutoEncoder_Dynamics, self).__init__()\n",
    "        \n",
    "        self.img_res = img_res\n",
    "        self.x_dim = img_res*img_res\n",
    "        self.z_dim = z_dim\n",
    "        self.u_dim = u_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('relu1', nn.Relu()),\n",
    "            ('conv2', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu2', nn.Relu()),\n",
    "            ('conv3', nn.Conv2d(8, 8, 5, padding=2)),\n",
    "            ('relu3', nn.Relu()),\n",
    "#             ('softmax', nn.softmax()),   # TODO: Implment and insert spatial softmax here...\n",
    "            ('fc1', nn.Linear(8*self.x_dim, 256)),\n",
    "            ('relu4', nn.Relu()),\n",
    "            ('dropout1', nn.Dropout(p=0.5)),\n",
    "            ('fc2', nn.Linear(256, 256)),\n",
    "            ('relu5', nn.Relu()),\n",
    "            ('fc3', nn.Linear(256, self.z_dim))\n",
    "        ]))\n",
    "        self.dynamics = nn.Sequential(OrderedDict([\n",
    "            ('d_fc1', nn.Linear(self.z_dim + self.u_dim, 128)),\n",
    "            ('d_relu1', nn.Relu()),\n",
    "            ('d_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('d_fc2', nn.Linear(128, 128)),\n",
    "            ('d_relu2', nn.Relu()),\n",
    "            ('d_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('d_fc3', nn.Linear(128, 128)),\n",
    "            ('d_relu3', nn.Relu())\n",
    "            ('d_fc4', nn.Linear(128, self.z_dim)),\n",
    "            ('d_relu4', nn.Relu())\n",
    "        ]))\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            ('dec_fc1', nn.Linear(self.z_dim, 512)),\n",
    "            ('dec_relu1', nn.Relu()),\n",
    "            ('dec_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc2', nn.Linear(512, 512)),\n",
    "            ('dec_relu2', nn.Relu()),\n",
    "            ('dec_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc3', nn.Linear(512, 512)),\n",
    "            ('dec_relu3', nn.Relu())\n",
    "            ('dec_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('dec_fc4', nn.Linear(512, 512)),\n",
    "            ('dec_relu4', nn.Relu())\n",
    "        ]))\n",
    "        self.environment = nn.Sequential(OrderedDict([\n",
    "            ('env_conv1', nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, padding=2)), # kernel_size different than original\n",
    "            ('env_relu1', nn.Relu()),\n",
    "            ('env_flat1', nn.Flatten()),\n",
    "            ('env_fc1', nn.Linear(4 * self.x_dim, 512)),\n",
    "            ('env_relu1', nn.Relu()),\n",
    "            ('env_dropout1', nn.Dropout(p=0.5)),\n",
    "            ('env_fc2', nn.Linear(512, 512)),\n",
    "            ('env_relu2', nn.Relu()),\n",
    "            ('env_dropout2', nn.Dropout(p=0.5)),\n",
    "            ('env_fc3', nn.Linear(512, 512)),\n",
    "            ('env_relu3', nn.Relu())\n",
    "            ('env_dropout3', nn.Dropout(p=0.5)),\n",
    "            ('env_fc4', nn.Linear(512, 512)),\n",
    "            ('env_relu4', nn.Relu())\n",
    "        ]))\n",
    "        self.last_layer = nn.Linear(512 + 512, self.x_dim)\n",
    "        \n",
    "    def forward(self, x_t, x_tplus, x_empty, u_t):\n",
    "        '''\n",
    "        x_t, x_tplus, x_empty must be of shape [N, C*H*W] where, N = batch_size, \n",
    "        C = Channels, H = Height, W = Width of image.\n",
    "        u is of shape [N, D_c] where D_c = Control Dimension.\n",
    "        '''\n",
    "        x_full = torch.cat((x_t, x_tplus), dim=0)\n",
    "        input_enc = torch.reshape(x_full, [-1, 1, self.img_res, self.img_res])\n",
    "        z_full = self.encoder(input_enc)\n",
    "        \n",
    "        z_t = z_full[:batch_size, :]\n",
    "        input_dyn = torch.cat((z_t, u_t), dim=1) #TODO: Do I have to use torch.identity after concatenation? why/not?\n",
    "        z_hat_tplus = self.dynamics(input_dyn)\n",
    "        \n",
    "        input_dec = torch.cat((z_t, z_hat_tplus), dim=0) #TODO: Again, should I use torch.identity here?\n",
    "        output_dec = self.decoder(input_dec)\n",
    "        \n",
    "        x_empty_full = torch.cat((x_empty, x_empty), dim=0)\n",
    "        input_env = torch.reshape(x_empty_full, [-1, 1, self.img_res, self.img_res]) #TODO: identity?\n",
    "        output_env = self.environment(input_env)\n",
    "        \n",
    "        input_last = torch.cat((output_dec, output_env), dim=1)\n",
    "        x_hat_full = self.last_layer(input_last)\n",
    "        \n",
    "        z_hat_tplus_zero = z_hat_tplus[:, 0]\n",
    "        z_hat_tplus_one = z_hat_tplus[:, 1]\n",
    "        grad_zh0_zt = torch.autograd.grad(z_hat_tplus_zero, z_t, grad_outputs=torch.ones(z_hat_tplus_zero.size()), retain_graph=True)\n",
    "        grad_zh1_zt = torch.autograd.grad(z_hat_tplus_one, z_t, grad_outputs=torch.ones(z_hat_tplus_one.size()), retain_graph=True)\n",
    "        grad_zh0_ut = torch.autograd.grad(z_hat_tplus_zero, u_t, grad_outputs=torch.ones(z_hat_tplus_zero.size()), retain_graph=True)\n",
    "        grad_zh1_ut = torch.autograd.grad(z_hat_tplus_one, u_t, grad_outputs=torch.ones(z_hat_tplus_one.size()), retain_graph=True)\n",
    "        \n",
    "        A = torch.stack([grad_zh0_zt, grad_zh1_zt], dim=1) # N x D_z_hat x D_z  (D_z_hat = D_z = 2)\n",
    "        B = torch.stack([grad_zh0_ut, grad_zh1_ut], dim=1) # N x D_z_hat x D_c  (D_c = 2)\n",
    "        c = self.__expand_dims(z_hat_tplus) - torch.bmm(A, self.__expand_dims(z_t)) - torch.bmm(B, self.__expand_dims(u_t))\n",
    "        AT = torch.transpose(A, 1, 2) # Preserve the batch dimension 0 and transpose dimentions 1 and 2\n",
    "        BT = torch.transpose(B, 1, 2)\n",
    "        \n",
    "        G = torch.bmm(A, torch.bmm(B, torch.bmm(BT, AT)))\n",
    "        \n",
    "    def __expand_dims(input):\n",
    "        return input.unsqueeze_(input.dim())\n",
    "        \n",
    "class CollisionChecker(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CollisionChecker, self).__init__()\n",
    "        # TODO Initialize Network Structure\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # TODO Define the forward pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pk-lsbmp",
   "language": "python",
   "name": "pk-lsbmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
